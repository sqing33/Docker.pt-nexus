# .github/workflows/douban_imdb_cf.yml

# 工作流的名称
name: Import Douban Data to Cloudflare D1

# 触发条件：当手动触发时运行 (workflow_dispatch)
on:
  workflow_dispatch:
    inputs:
      database_name:
        description: 'Your D1 Database Name'
        required: true
        default: 'pt-nexus' # 您的D1数据库名
      json_url:
        description: 'URL of the JSON file to import'
        required: true
        default: 'https://raw.githubusercontent.com/ourbits/PtGen/refs/heads/main/internal_map/douban_imdb_map.json'

# 定义一个任务 (job)
jobs:
  import-data:
    # 任务运行在 GitHub 提供的最新版 Ubuntu 虚拟服务器上
    runs-on: ubuntu-latest

    # 任务的步骤
    steps:
      # 步骤 1: 检出代码 (虽然我们没有代码，但这是标准步骤)
      - name: Checkout repository
        uses: actions/checkout@v4

      # 步骤 2: 安装 Node.js
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20' # 使用较新的 Node.js 版本

      # 步骤 3: 安装 Cloudflare Wrangler CLI
      - name: Install Wrangler
        run: npm install -g wrangler

      # 步骤 4: 下载 JSON 数据文件
      - name: Download JSON data
        run: |
          echo "Downloading data from ${{ github.event.inputs.json_url }}..."
          curl -o data.json "${{ github.event.inputs.json_url }}"
          echo "Download complete."

      # 步骤 5: 创建并执行导入脚本
      - name: Create and run import script
        # 将 Cloudflare 的凭证设置为环境变量，这样 Wrangler 才能使用
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          # 使用 Node.js 动态生成并执行导入脚本
          node -e "
            const fs = require('fs');
            const { execSync } = require('child_process');

            const DATABASE_NAME = '${{ github.event.inputs.database_name }}';
            const INPUT_FILE = './data.json';
            const BATCH_SIZE = 200;

            async function main() {
              console.log('[步骤 1/3] 读取本地已下载的数据...');
              const content = fs.readFileSync(INPUT_FILE, 'utf-8');
              const allData = JSON.parse(content);
              console.log(`读取成功! 共 ${allData.length} 条记录。`);

              console.log('[步骤 2/3] (可选) 清空云端的数据表...');
              try {
                execSync(`wrangler d1 execute ${DATABASE_NAME} --command 'DELETE FROM \"douban-imdb\";'`, { stdio: 'inherit' });
                console.log('云端数据表已清空。');
              } catch (e) {
                console.error('清空数据表失败:', e.message);
              }

              console.log('[步骤 3/3] 开始分批导入数据到云端...');
              for (let i = 0; i < allData.length; i += BATCH_SIZE) {
                const batch = allData.slice(i, i + BATCH_SIZE);
                const currentBatchNum = i / BATCH_SIZE + 1;
                
                const sqlValues = batch.map(() => '(?, ?, ?, ?)').join(',');
                const fullSql = \`INSERT OR IGNORE INTO \\"douban-imdb\\" (doubanid, imdbid, name, year) VALUES \${sqlValues}\`;
                const bindings = batch.flatMap(item => [item.dbid, item.imdbid, item.name, item.year]);
                
                const query = { sql: fullSql, params: bindings };
                fs.writeFileSync('./query.json', JSON.stringify(query));

                try {
                  console.log(\`正在推送第 \${currentBatchNum} 批数据...\`);
                  execSync(\`wrangler d1 execute ${DATABASE_NAME} --json-file=./query.json\`, { stdio: 'inherit' });
                } catch (e) {
                  console.error(\`第 \${currentBatchNum} 批数据推送失败:\`, e.message);
                  process.exit(1);
                }
              }

              console.log('🎉 全部数据导入成功!');
            }

            main();
          "
