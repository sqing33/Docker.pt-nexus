# .github/workflows/douban_imdb_cf.yml

name: 更新 cf 豆瓣-IMDb 映射数据库

on:
  workflow_dispatch:

  schedule:
    - cron: '0 0 * * 0'

jobs:
  import-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Latest Wrangler
        run: npm install -g wrangler@latest

      - name: Create Wrangler Config File
        run: |
          DB_NAME="pt-nexus"
          
          echo "name = \"d1-importer-project\"" > wrangler.toml
          echo "compatibility_date = \"$(date +'%Y-%m-%d')\"" >> wrangler.toml
          echo "" >> wrangler.toml
          echo "[[d1_databases]]" >> wrangler.toml
          echo "binding = \"$DB_NAME\"" >> wrangler.toml
          echo "database_name = \"$DB_NAME\"" >> wrangler.toml
          echo "database_id = \"${{ secrets.CLOUDFLARE_D1_DATABASE_ID }}\"" >> wrangler.toml
          
          echo "Wrangler config file created:"
          cat wrangler.toml

      - name: Download JSON data
        run: |
          JSON_URL="https://raw.githubusercontent.com/ourbits/PtGen/refs/heads/main/internal_map/douban_imdb_map.json"
          curl -o data.json "$JSON_URL"

      - name: Create and run import script
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          cat <<'EOF' > import.mjs
          import fs from 'fs';
          import { execSync } from 'child_process';

          const DATABASE_NAME = 'pt-nexus';
          const INPUT_FILE = './data.json';
          const BATCH_SIZE = 1000;

          function escapeSqlValue(value) {
            if (value === null || typeof value === 'undefined') {
              return 'NULL';
            }
            return `'${String(value).replace(/'/g, "''")}'`;
          }

          async function main() {
            console.log(`[步骤 1/2] 读取本地已下载的数据...`);
            const content = fs.readFileSync(INPUT_FILE, 'utf-8');
            const allData = JSON.parse(content);
            console.log(`读取成功! 共 ${allData.length} 条记录。`);

            console.log(`[步骤 2/2] 开始分批导入数据到云端...`);
            for (let i = 0; i < allData.length; i += BATCH_SIZE) {
              const batch = allData.slice(i, i + BATCH_SIZE);
              const currentBatchNum = i / BATCH_SIZE + 1;
              const totalBatches = Math.ceil(allData.length / BATCH_SIZE);
              
              const valuesString = batch.map(item => 
                `(${escapeSqlValue(item.dbid)}, ${escapeSqlValue(item.imdbid)}, ${escapeSqlValue(item.name)}, ${escapeSqlValue(item.year)})`
              ).join(',');

              if (!valuesString) continue;

              const fullSql = `INSERT OR IGNORE INTO "douban-imdb" (doubanid, imdbid, name, year) VALUES ${valuesString};`;
              const escapedSql = fullSql.replace(/'/g, "'\\''");

              const command = `wrangler d1 execute ${DATABASE_NAME} --remote --command '${escapedSql}'`;
              
              try {
                console.log(`正在推送第 ${currentBatchNum} / ${totalBatches} 批数据...`);
                execSync(command, { stdio: 'inherit' });
              } catch (e) {
                console.error(`第 ${currentBatchNum} 批数据推送失败:`, e.message);
                process.exit(1);
              }
            }

            console.log('🎉 全部数据导入/更新成功!');
          }

          main();
          EOF
          
          node import.mjs
