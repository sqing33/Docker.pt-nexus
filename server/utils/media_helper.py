# utils/media_helper.py

import base64
import logging
import mimetypes
import re
import os
import shutil
import subprocess
import tempfile
import requests
import json
import time
import random
import cloudscraper
import yaml
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pymediainfo import MediaInfo
from config import TEMP_DIR, config_manager, GLOBAL_MAPPINGS
from qbittorrentapi import Client as qbClient
from transmission_rpc import Client as TrClient
from utils import ensure_scheme
from PIL import Image


def translate_path(downloader_id: str, remote_path: str) -> str:
    """
    å°†ä¸‹è½½å™¨çš„è¿œç¨‹è·¯å¾„è½¬æ¢ä¸º PT Nexus å®¹å™¨å†…çš„æœ¬åœ°è·¯å¾„ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :param remote_path: ä¸‹è½½å™¨ä¸­çš„è¿œç¨‹è·¯å¾„
    :return: PT Nexus å®¹å™¨å†…å¯è®¿é—®çš„æœ¬åœ°è·¯å¾„
    """
    if not downloader_id or not remote_path:
        return remote_path

    # è·å–ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            path_mappings = downloader.get("path_mappings", [])
            if not path_mappings:
                # æ²¡æœ‰é…ç½®è·¯å¾„æ˜ å°„ï¼Œç›´æ¥è¿”å›åŸè·¯å¾„
                return remote_path

            # æŒ‰è¿œç¨‹è·¯å¾„é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…æœ€é•¿çš„è·¯å¾„ï¼ˆæ›´ç²¾ç¡®ï¼‰
            sorted_mappings = sorted(path_mappings,
                                     key=lambda x: len(x.get('remote', '')),
                                     reverse=True)

            for mapping in sorted_mappings:
                remote = mapping.get('remote', '')
                local = mapping.get('local', '')

                if not remote or not local:
                    continue

                # ç¡®ä¿è·¯å¾„æ¯”è¾ƒæ—¶ç»Ÿä¸€å¤„ç†æœ«å°¾çš„æ–œæ 
                remote = remote.rstrip('/')
                remote_path_normalized = remote_path.rstrip('/')

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é…æˆ–å‰ç¼€åŒ¹é…ï¼‰
                if remote_path_normalized == remote:
                    # å®Œå…¨åŒ¹é…
                    return local
                elif remote_path_normalized.startswith(remote + '/'):
                    # å‰ç¼€åŒ¹é…ï¼Œæ›¿æ¢è·¯å¾„
                    relative_path = remote_path_normalized[len(remote
                                                               ):].lstrip('/')
                    return os.path.join(local, relative_path)

            # æ²¡æœ‰åŒ¹é…çš„æ˜ å°„ï¼Œè¿”å›åŸè·¯å¾„
            return remote_path

    # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ä¸‹è½½å™¨ï¼Œè¿”å›åŸè·¯å¾„
    return remote_path


def _upload_to_pixhost(image_path: str):
    """
    å°†å•ä¸ªå›¾ç‰‡æ–‡ä»¶ä¸Šä¼ åˆ° Pixhost.toï¼Œæ”¯æŒä¸»å¤‡åŸŸååˆ‡æ¢ã€‚

    :param image_path: æœ¬åœ°å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„ã€‚
    :return: æˆåŠŸæ—¶è¿”å›å›¾ç‰‡çš„å±•ç¤ºURLï¼Œå¤±è´¥æ—¶è¿”å›Noneã€‚
    """
    # ä¸»å¤‡åŸŸåé…ç½® - æ›¿æ¢å­åŸŸåéƒ¨åˆ†
    api_urls = [
        'http://ptn-proxy.sqing33.dpdns.org/https://api.pixhost.to/images',
        'http://ptn-proxy.1395251710.workers.dev/https://api.pixhost.to/images'
    ]

    params = {'content_type': 0}
    headers = {
        'User-Agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡: {image_path}")

    if not os.path.exists(image_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {image_path}")
        return None

    # å°è¯•ä½¿ç”¨ä¸åŒçš„API URL
    for i, api_url in enumerate(api_urls):
        domain_name = "ä¸»åŸŸå" if i == 0 else "å¤‡ç”¨åŸŸå"
        print(f"å°è¯•ä½¿ç”¨{domain_name}: {api_url}")

        result = _upload_to_pixhost_direct(image_path, api_url, params, headers)
        if result:
            print(f"{domain_name}ä¸Šä¼ æˆåŠŸ")
            return result
        else:
            print(f"{domain_name}ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª")

    print("æ‰€æœ‰APIåŸŸåéƒ½ä¸Šä¼ å¤±è´¥")
    return None


def _get_agsv_auth_token():
    """ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‚®ç®±å’Œå¯†ç è·å– æœ«æ—¥å›¾åºŠ çš„æˆæƒ Tokenã€‚"""
    config = config_manager.get().get("cross_seed", {})
    email = config.get("agsv_email")
    password = config.get("agsv_password")

    if not email or not password:
        logging.warning("æœ«æ—¥å›¾åºŠ é‚®ç®±æˆ–å¯†ç æœªé…ç½®ï¼Œæ— æ³•è·å– Tokenã€‚")
        return None

    token_url = "https://img.seedvault.cn/api/v1/tokens"
    payload = {"email": email, "password": password}
    headers = {"Accept": "application/json"}
    print("æ­£åœ¨ä¸º æœ«æ—¥å›¾åºŠ è·å–æˆæƒ Token...")
    try:
        response = requests.post(token_url,
                                 headers=headers,
                                 json=payload,
                                 timeout=30)
        if response.status_code == 200 and response.json().get("status"):
            token = response.json().get("data", {}).get("token")
            if token:
                print("   âœ… æˆåŠŸè·å– æœ«æ—¥å›¾åºŠ Tokenï¼")
                return token

        logging.error(
            f"è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"
        )
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥: {response.text}")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None


def _upload_to_agsv(image_path: str, token: str):
    """ä½¿ç”¨ç»™å®šçš„ Token ä¸Šä¼ å•ä¸ªå›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠã€‚"""
    upload_url = "https://img.seedvault.cn/api/v1/upload"
    headers = {
        "Authorization":
        f"Bearer {token}",
        "Accept":
        "application/json",
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    }

    mime_type = mimetypes.guess_type(
        image_path)[0] or 'application/octet-stream'
    image_name = os.path.basename(image_path)

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠ: {image_name}")
    try:
        with open(image_path, 'rb') as f:
            files = {'file': (image_name, f, mime_type)}
            response = requests.post(upload_url,
                                     headers=headers,
                                     files=files,
                                     timeout=180)

        data = response.json()
        if response.status_code == 200 and data.get("status"):
            image_url = data.get("data", {}).get("links", {}).get("url")
            print(f"   âœ… æœ«æ—¥å›¾åºŠ ä¸Šä¼ æˆåŠŸï¼URL: {image_url}")
            return image_url
        else:
            message = data.get('message', 'æ— è¯¦ç»†ä¿¡æ¯')
            logging.error(f"æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥ã€‚API æ¶ˆæ¯: {message}")
            print(f"   âŒ æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥: {message}")
            return None
    except (requests.exceptions.RequestException,
            requests.exceptions.JSONDecodeError) as e:
        logging.error(f"ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"   âŒ ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def _get_smart_screenshot_points(video_path: str,
                                 num_screenshots: int = 5) -> list[float]:
    """
    [ä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ ffprobe æ™ºèƒ½åˆ†æè§†é¢‘å­—å¹•ï¼Œé€‰æ‹©æœ€ä½³çš„æˆªå›¾æ—¶é—´ç‚¹ã€‚
    - é€šè¿‡ `-read_intervals` å‚æ•°å®ç°åˆ†æ®µè¯»å–ï¼Œé¿å…å…¨æ–‡ä»¶æ‰«æï¼Œå¤§å¹…æå‡å¤§æ–‡ä»¶å¤„ç†é€Ÿåº¦ã€‚
    - ä¼˜å…ˆé€‰æ‹© ASS > SRT > PGS æ ¼å¼çš„å­—å¹•ã€‚
    - ä¼˜å…ˆåœ¨è§†é¢‘çš„ 30%-80% "é»„é‡‘æ—¶æ®µ" å†…éšæœºé€‰æ‹©ã€‚
    - åœ¨æ‰€æœ‰æ™ºèƒ½åˆ†æå¤±è´¥æ—¶ï¼Œä¼˜é›…åœ°å›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”é€‰æ‹©ã€‚
    """
    print("\n--- å¼€å§‹æ™ºèƒ½æˆªå›¾æ—¶é—´ç‚¹åˆ†æ (å¿«é€Ÿæ‰«ææ¨¡å¼) ---")
    if not shutil.which("ffprobe"):
        print("è­¦å‘Š: æœªæ‰¾åˆ° ffprobeï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åˆ†æã€‚")
        return []

    try:
        cmd_duration = [
            "ffprobe", "-v", "error", "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1", video_path
        ]
        result = subprocess.run(cmd_duration,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        duration = float(result.stdout.strip())
        print(f"è§†é¢‘æ€»æ—¶é•¿: {duration:.2f} ç§’")
    except Exception as e:
        print(f"é”™è¯¯ï¼šä½¿ç”¨ ffprobe è·å–è§†é¢‘æ—¶é•¿å¤±è´¥ã€‚{e}")
        return []

    # æ¢æµ‹å­—å¹•æµçš„éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒæœ¬èº«é€Ÿåº¦å¾ˆå¿«
    try:
        cmd_probe_subs = [
            "ffprobe", "-v", "quiet", "-print_format", "json", "-show_entries",
            "stream=index,codec_name,disposition", "-select_streams", "s",
            video_path
        ]
        result = subprocess.run(cmd_probe_subs,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        sub_data = json.loads(result.stdout)

        best_ass, best_srt, best_pgs = None, None, None
        for stream in sub_data.get("streams", []):
            disposition = stream.get("disposition", {})
            is_normal = not any([
                disposition.get("comment"),
                disposition.get("hearing_impaired"),
                disposition.get("visual_impaired")
            ])
            if is_normal:
                codec_name = stream.get("codec_name")
                if codec_name == "ass" and not best_ass: best_ass = stream
                elif codec_name == "subrip" and not best_srt: best_srt = stream
                elif codec_name == "hdmv_pgs_subtitle" and not best_pgs:
                    best_pgs = stream

        chosen_sub_stream = best_ass or best_srt or best_pgs
        if not chosen_sub_stream:
            print("æœªæ‰¾åˆ°åˆé€‚çš„æ­£å¸¸å­—å¹•æµã€‚")
            return []

        sub_index, sub_codec = chosen_sub_stream.get(
            "index"), chosen_sub_stream.get("codec_name")
        print(f"   âœ… æ‰¾åˆ°æœ€ä¼˜å­—å¹•æµ (æ ¼å¼: {sub_codec.upper()})ï¼Œæµç´¢å¼•: {sub_index}")

    except Exception as e:
        print(f"æ¢æµ‹å­—å¹•æµå¤±è´¥: {e}")
        return []

    subtitle_events = []
    try:
        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ ---
        # 1. å®šä¹‰æˆ‘ä»¬è¦æ¢æµ‹çš„æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘çš„20%, 40%, 60%, 80%ä½ç½®ï¼‰
        probe_points = [0.2, 0.4, 0.6, 0.8]
        # 2. å®šä¹‰åœ¨æ¯ä¸ªæ¢æµ‹ç‚¹é™„è¿‘æ‰«æå¤šé•¿æ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œ60ç§’ï¼‰ï¼Œæ—¶é—´è¶Šé•¿ï¼Œæ‰¾åˆ°å­—å¹•äº‹ä»¶è¶Šå¤šï¼Œä½†è€—æ—¶ä¹Ÿè¶Šé•¿
        probe_duration = 60

        # 3. æ„å»º -read_intervals å‚æ•°
        # æ ¼å¼ä¸º "start1%+duration1,start2%+duration2,..."
        intervals = []
        for point in probe_points:
            start_time = duration * point
            end_time = start_time + probe_duration
            if end_time > duration:
                end_time = duration  # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘æ€»é•¿
            intervals.append(f"{start_time}%{end_time}")

        read_intervals_arg = ",".join(intervals)
        print(f"   ğŸš€ å°†åªæ‰«æä»¥ä¸‹æ—¶é—´æ®µæ¥å¯»æ‰¾å­—å¹•: {read_intervals_arg}")

        # 4. å°† -read_intervals å‚æ•°æ·»åŠ åˆ° ffprobe å‘½ä»¤ä¸­
        cmd_extract = [
            "ffprobe",
            "-v",
            "quiet",
            "-read_intervals",
            read_intervals_arg,  # <--- æ–°å¢çš„å‚æ•°
            "-print_format",
            "json",
            "-show_packets",
            "-select_streams",
            str(sub_index),
            video_path
        ]

        # æ‰§è¡Œå‘½ä»¤ï¼Œç°åœ¨å®ƒä¼šå¿«éå¸¸å¤š
        result = subprocess.run(cmd_extract,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        # --- ã€æ ¸å¿ƒä¿®æ”¹ç»“æŸã€‘ ---

        events_data = json.loads(result.stdout)
        packets = events_data.get("packets", [])

        # åç»­å¤„ç†é€»è¾‘åŸºæœ¬ä¸å˜
        if sub_codec in ["ass", "subrip"]:
            for packet in packets:
                try:
                    start, dur = float(packet.get("pts_time")), float(
                        packet.get("duration_time"))
                    if dur > 0.1:
                        subtitle_events.append({
                            "start": start,
                            "end": start + dur
                        })
                except (ValueError, TypeError):
                    continue
        elif sub_codec == "hdmv_pgs_subtitle":
            for i in range(0, len(packets) - 1, 2):
                try:
                    start, end = float(packets[i].get("pts_time")), float(
                        packets[i + 1].get("pts_time"))
                    if end > start and (end - start) > 0.1:
                        subtitle_events.append({"start": start, "end": end})
                except (ValueError, TypeError):
                    continue

        if not subtitle_events: raise ValueError("åœ¨æŒ‡å®šåŒºé—´å†…æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ—¶é—´äº‹ä»¶ã€‚")
        print(f"   âœ… æˆåŠŸä»æŒ‡å®šåŒºé—´æå–åˆ° {len(subtitle_events)} æ¡æœ‰æ•ˆå­—å¹•äº‹ä»¶ã€‚")
    except Exception as e:
        print(f"æ™ºèƒ½æå–æ—¶é—´äº‹ä»¶å¤±è´¥: {e}")
        return []

    # åç»­çš„éšæœºé€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
    if len(subtitle_events) < num_screenshots:
        print("æœ‰æ•ˆå­—å¹•æ•°é‡ä¸è¶³ï¼Œæ— æ³•å¯åŠ¨æ™ºèƒ½é€‰æ‹©ã€‚")
        return []

    golden_start_time, golden_end_time = duration * 0.30, duration * 0.80
    golden_events = [
        e for e in subtitle_events
        if e["start"] >= golden_start_time and e["end"] <= golden_end_time
    ]
    print(
        f"   -> åœ¨è§†é¢‘ä¸­éƒ¨ ({(golden_start_time):.2f}s - {(golden_end_time):.2f}s) æ‰¾åˆ° {len(golden_events)} ä¸ªé»„é‡‘å­—å¹•äº‹ä»¶ã€‚"
    )

    target_events = golden_events
    if len(target_events) < num_screenshots:
        print("   -> é»„é‡‘å­—å¹•æ•°é‡ä¸è¶³ï¼Œå°†ä»æ‰€æœ‰å­—å¹•äº‹ä»¶ä¸­éšæœºé€‰æ‹©ã€‚")
        target_events = subtitle_events

    chosen_events = random.sample(target_events,
                                  min(num_screenshots, len(target_events)))

    screenshot_points = []
    for event in chosen_events:
        event_duration = event["end"] - event["start"]
        random_offset = event_duration * 0.1 + random.random() * (
            event_duration * 0.8)
        random_point = event["start"] + random_offset
        screenshot_points.append(random_point)
        print(
            f"   -> é€‰ä¸­æ—¶é—´æ®µ [{(event['start']):.2f}s - {(event['end']):.2f}s], éšæœºæˆªå›¾ç‚¹: {(random_point):.2f}s"
        )

    return sorted(screenshot_points)


def _find_target_video_file(path: str) -> tuple[str | None, bool]:
    """
    æ ¹æ®è·¯å¾„æ™ºèƒ½æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶ï¼Œå¹¶æ£€æµ‹æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ã€‚
    - ä¼˜å…ˆæ£€æŸ¥ç§å­åç§°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¤„ç†ç”µå½±ç›´æ¥æ”¾åœ¨ä¸‹è½½ç›®å½•æ ¹ç›®å½•çš„æƒ…å†µï¼‰
    - å¦‚æœæ˜¯ç”µå½±ç›®å½•ï¼Œè¿”å›æœ€å¤§çš„è§†é¢‘æ–‡ä»¶ã€‚
    - å¦‚æœæ˜¯å‰§é›†ç›®å½•ï¼Œè¿”å›æŒ‰åç§°æ’åºçš„ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚
    - æ£€æµ‹æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ï¼ˆæ£€æŸ¥ BDMV/CERTIFICATE ç›®å½•ï¼‰

    :param path: è¦æœç´¢çš„ç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ã€‚
    :return: å…ƒç»„ (ç›®æ ‡è§†é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„, æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶)
    """
    print(f"å¼€å§‹åœ¨è·¯å¾„ '{path}' ä¸­æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶...")
    VIDEO_EXTENSIONS = {
        ".mkv", ".mp4", ".ts", ".avi", ".wmv", ".mov", ".flv", ".m2ts"
    }

    if not os.path.exists(path):
        print(f"é”™è¯¯ï¼šæä¾›çš„è·¯å¾„ä¸å­˜åœ¨: {path}")
        return None, False

    # å¦‚æœæä¾›çš„è·¯å¾„æœ¬èº«å°±æ˜¯ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(path) and os.path.splitext(
            path)[1].lower() in VIDEO_EXTENSIONS:
        print(f"è·¯å¾„ç›´æ¥æŒ‡å‘ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†ä½¿ç”¨: {path}")
        return path, False

    if not os.path.isdir(path):
        print(f"é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•æˆ–è§†é¢‘æ–‡ä»¶: {path}")
        return None, False

    # æ£€æŸ¥æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ï¼ˆæ£€æŸ¥ BDMV/CERTIFICATE ç›®å½•ï¼‰
    is_bluray_disc = False
    bdmv_path = os.path.join(path, "BDMV")
    certificate_path = os.path.join(path, "CERTIFICATE")

    if os.path.exists(bdmv_path) and os.path.isdir(bdmv_path):
        print(f"æ£€æµ‹åˆ° BDMV ç›®å½•: {bdmv_path}")
        if certificate_path and os.path.exists(
                certificate_path) and os.path.isdir(certificate_path):
            print(f"æ£€æµ‹åˆ° CERTIFICATE ç›®å½•: {certificate_path}")
            is_bluray_disc = True
            print("ç¡®è®¤ï¼šæ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ (BDMV/CERTIFICATE)")
        else:
            print("è­¦å‘Šï¼šæ£€æµ‹åˆ° BDMV ç›®å½•ä½†æœªæ‰¾åˆ° CERTIFICATE ç›®å½•ï¼Œå¯èƒ½ä¸æ˜¯æ ‡å‡†åŸç›˜")

    # ä¼˜å…ˆæ£€æŸ¥ç§å­åç§°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¤„ç†ç”µå½±ç›´æ¥æ”¾åœ¨æ ¹ç›®å½•çš„æƒ…å†µï¼‰
    # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨æ²¡æœ‰æ–‡ä»¶å¤¹åŒ…è£¹çš„ç”µå½±æ–‡ä»¶
    parent_dir = os.path.dirname(path)
    file_name = os.path.basename(path)

    # æ£€æŸ¥çˆ¶ç›®å½•ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    if parent_dir != path:  # ç¡®ä¿è¿™ä¸æ˜¯æ ¹ç›®å½•çš„æƒ…å†µ
        try:
            for file in os.listdir(parent_dir):
                if not file.startswith('.') and not os.path.isdir(
                        os.path.join(parent_dir, file)):
                    if os.path.splitext(file)[1].lower() in VIDEO_EXTENSIONS:
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…ï¼ˆå¿½ç•¥æ‰©å±•åï¼‰
                        file_name_without_ext = os.path.splitext(file)[0]
                        if (file_name in file_name_without_ext
                                or file_name_without_ext in file_name
                                or file_name.replace(' ', '')
                                in file_name_without_ext.replace(' ', '')
                                or file_name_without_ext.replace(
                                    ' ', '') in file_name.replace(' ', '')):
                            full_path = os.path.join(parent_dir, file)
                            print(f"æ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {full_path}")
                            return full_path, is_bluray_disc
        except OSError as e:
            print(f"è¯»å–çˆ¶ç›®å½•å¤±è´¥: {e}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼Œç»§ç»­åŸæ¥çš„æŸ¥æ‰¾é€»è¾‘
    video_files = []
    for root, _, files in os.walk(path):
        for file in files:
            if os.path.splitext(file)[1].lower() in VIDEO_EXTENSIONS:
                video_files.append(os.path.join(root, file))

    if not video_files:
        print(f"åœ¨ç›®å½• '{path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶ã€‚")
        return None, is_bluray_disc

    # å¦‚æœåªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
    if len(video_files) == 1:
        print(f"æ‰¾åˆ°å”¯ä¸€çš„è§†é¢‘æ–‡ä»¶: {video_files[0]}")
        return video_files[0], is_bluray_disc

    # å¦‚æœæœ‰å¤šä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°è¯•æ‰¾åˆ°æœ€åŒ¹é…çš„æ–‡ä»¶å
    best_match = ""
    best_score = -1
    for video_file in video_files:
        base_name = os.path.basename(video_file).lower()
        path_name = file_name.lower()

        # è®¡ç®—åŒ¹é…åº¦
        score = 0
        if path_name in base_name:
            score += 10
        if base_name in path_name:
            score += 5

        # é•¿åº¦è¶Šæ¥è¿‘ï¼Œå¾—åˆ†è¶Šé«˜
        if abs(len(base_name) - len(path_name)) < 5:
            score += 3

        if score > best_score:
            best_score = score
            best_match = video_file

    if best_match and best_score > 0:
        print(f"é€‰æ‹©æœ€ä½³åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {best_match} (åŒ¹é…åº¦: {best_score})")
        return best_match, is_bluray_disc

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åŒ¹é…ï¼Œé€‰æ‹©æœ€å¤§çš„æ–‡ä»¶
    largest_file = ""
    max_size = -1
    for f in video_files:
        try:
            size = os.path.getsize(f)
            if size > max_size:
                max_size = size
                largest_file = f
        except OSError as e:
            print(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° '{f}': {e}")
            continue

    if largest_file:
        print(f"å·²é€‰æ‹©æœ€å¤§æ–‡ä»¶ ({(max_size / 1024**3):.2f} GB): {largest_file}")
        return largest_file, is_bluray_disc
    else:
        print("æ— æ³•ç¡®å®šæœ€å¤§çš„æ–‡ä»¶ã€‚")
        return None, is_bluray_disc


def validate_media_info_format(mediaInfo: str):
    """
    éªŒè¯ MediaInfo æˆ– BDInfo æ ¼å¼çš„æœ‰æ•ˆæ€§
    ä» global_mappings.yaml è¯»å–é…ç½®çš„å…³é”®å­—è¿›è¡ŒéªŒè¯
    """
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å…³é”®å­—é…ç½®
    try:
        with open(GLOBAL_MAPPINGS, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        mediainfo_keywords = config.get('content_filtering',
                                        {}).get('mediainfo_keywords', {})
        bdinfo_keywords = config.get('content_filtering',
                                     {}).get('bdinfo_keywords', {})

        mediainfo_required = mediainfo_keywords.get('required', [])
        mediainfo_optional = mediainfo_keywords.get('optional', [])
        bdinfo_required = bdinfo_keywords.get('required', [])
        bdinfo_optional = bdinfo_keywords.get('optional', [])
    except Exception as e:
        print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤é…ç½®
        mediainfo_required = ["General", "Video", "Audio"]
        mediainfo_optional = [
            "Complete name", "File size", "Duration", "Width", "Height"
        ]
        bdinfo_required = ["DISC INFO", "PLAYLIST REPORT"]
        bdinfo_optional = [
            "VIDEO:", "AUDIO:", "SUBTITLES:", "FILES:", "Disc Label",
            "Disc Size", "BDInfo:", "Protection:", "Codec", "Bitrate",
            "Language", "Description"
        ]

    # éªŒè¯ MediaInfo æ ¼å¼
    mediainfo_required_matches = sum(1 for keyword in mediainfo_required
                                     if keyword in mediaInfo)
    mediainfo_optional_matches = sum(1 for keyword in mediainfo_optional
                                     if keyword in mediaInfo)
    is_mediainfo = (len(mediainfo_required) > 0 and
                            mediainfo_required_matches == len(mediainfo_required) and
                            mediainfo_optional_matches >= 0) or \
                           (mediainfo_required_matches >= 2 and
                            mediainfo_optional_matches >= 1)

    # éªŒè¯ BDInfo æ ¼å¼
    bdinfo_required_matches = sum(1 for keyword in bdinfo_required
                                  if keyword in mediaInfo)
    bdinfo_optional_matches = sum(1 for keyword in bdinfo_optional
                                  if keyword in mediaInfo)
    is_bdinfo = (len(bdinfo_required) > 0 and bdinfo_required_matches == len(bdinfo_required)) or \
                (bdinfo_required_matches >= 1 and bdinfo_optional_matches >= 2)

    return is_mediainfo, is_bdinfo, mediainfo_required_matches, mediainfo_optional_matches, bdinfo_required_matches, bdinfo_optional_matches


# --- [ä¿®æ”¹] ä¸»å‡½æ•°ï¼Œæ•´åˆäº†æ–°çš„æ–‡ä»¶æŸ¥æ‰¾é€»è¾‘ ---
def upload_data_mediaInfo(mediaInfo: str,
                          save_path: str,
                          content_name: str = None,
                          downloader_id: str = None,
                          torrent_name: str = None,
                          force_refresh: bool = False):
    """
    æ£€æŸ¥ä¼ å…¥çš„æ–‡æœ¬æ˜¯æœ‰æ•ˆçš„ MediaInfo è¿˜æ˜¯ BDInfo æ ¼å¼ã€‚
    å¦‚æœæ²¡æœ‰ MediaInfo æˆ– BDInfo åˆ™å°è¯•ä» save_path æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶æå– MediaInfoã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ torrent_name (å®é™…æ–‡ä»¶å¤¹å) æˆ– content_name (è§£æåçš„æ ‡é¢˜) æ¥æ„å»ºæ›´ç²¾ç¡®çš„æœç´¢è·¯å¾„ã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ downloader_id æ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä»£ç†è·å– MediaInfo
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ force_refresh å¼ºåˆ¶é‡æ–°è·å– MediaInfoï¼Œå¿½ç•¥å·²æœ‰çš„æœ‰æ•ˆæ ¼å¼
    """
    print("å¼€å§‹æ£€æŸ¥ MediaInfo/BDInfo æ ¼å¼")

    # ä½¿ç”¨æ–°çš„éªŒè¯å‡½æ•°è¿›è¡Œæ ¼å¼éªŒè¯
    is_mediainfo, is_bdinfo, mediainfo_required_matches, mediainfo_optional_matches, bdinfo_required_matches, bdinfo_optional_matches = validate_media_info_format(
        mediaInfo)

    if is_mediainfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(
                f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(å¿…è¦å…³é”®å­—: {mediainfo_required_matches}/2, åŒ¹é…å…³é”®å­—æ•°: {mediainfo_required_matches + mediainfo_optional_matches})"
            )
            return mediaInfo, True, False
    elif is_bdinfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(
                f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(å¿…è¦å…³é”®å­—: {bdinfo_required_matches}/2, å¯é€‰å…³é”®å­—: {bdinfo_required_matches + bdinfo_optional_matches})"
            )
            return mediaInfo, False, True
    elif not force_refresh:
        # åªæœ‰åœ¨ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°æ—¶æ‰æ‰“å°è¿™ä¸ªæ¶ˆæ¯
        print("æä¾›çš„æ–‡æœ¬ä¸æ˜¯æœ‰æ•ˆçš„ MediaInfo/BDInfoï¼Œå°†å°è¯•ä»æœ¬åœ°æ–‡ä»¶æå–ã€‚")

    # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜éœ€è¦é‡æ–°æå–ï¼ˆforce_refresh=True æˆ–è€…æ²¡æœ‰æœ‰æ•ˆæ ¼å¼ï¼‰
    if not save_path:
        print("é”™è¯¯ï¼šæœªæä¾› save_pathï¼Œæ— æ³•ä»æ–‡ä»¶æå– MediaInfoã€‚")
        return mediaInfo, False, False

    # --- ã€ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ã€‘ ---
    proxy_config = _get_downloader_proxy_config(downloader_id)

    if proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç† MediaInfo: {proxy_config['proxy_base_url']}")
        # æ„å»ºå®Œæ•´è·¯å¾„å‘é€ç»™ä»£ç†
        remote_path = save_path
        if torrent_name:
            remote_path = os.path.join(save_path, torrent_name)
            print(f"å·²æä¾› torrent_nameï¼Œå°†ä½¿ç”¨å®Œæ•´è·¯å¾„: '{remote_path}'")
        elif content_name:
            remote_path = os.path.join(save_path, content_name)
            print(f"å·²æä¾› content_nameï¼Œå°†ä½¿ç”¨æ‹¼æ¥è·¯å¾„: '{remote_path}'")

        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/mediainfo",
                json={"remote_path": remote_path},
                timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("é€šè¿‡ä»£ç†è·å– MediaInfo æˆåŠŸ")
                proxy_mediainfo = result.get("mediainfo", mediaInfo)
                # å¤„ç†ä»£ç†è¿”å›çš„ MediaInfoï¼Œåªä¿ç•™ Complete name ä¸­çš„æ–‡ä»¶å
                proxy_mediainfo = re.sub(
                    r'(Complete name\s*:\s*)(.+)', lambda m:
                    f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
                    proxy_mediainfo)
                return proxy_mediainfo, True, False
            else:
                print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {e}")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä»¿ç…§æˆªå›¾é€»è¾‘ï¼Œæ„å»ºç²¾ç¡®çš„æœç´¢è·¯å¾„ ---
    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    path_to_search = translated_save_path  # ä½¿ç”¨è½¬æ¢åçš„è·¯å¾„
    # ä¼˜å…ˆä½¿ç”¨ torrent_name (å®é™…æ–‡ä»¶å¤¹å)ï¼Œå¦‚æœä¸å­˜åœ¨å†ä½¿ç”¨ content_name (è§£æåçš„æ ‡é¢˜)
    if torrent_name:
        path_to_search = os.path.join(translated_save_path, torrent_name)
        print(f"å·²æä¾› torrent_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")
    elif content_name:
        # å¦‚æœæä¾›äº†å…·ä½“çš„å†…å®¹åç§°ï¼ˆä¸»æ ‡é¢˜ï¼‰ï¼Œåˆ™æ‹¼æ¥æˆä¸€ä¸ªæ›´ç²¾ç¡®çš„è·¯å¾„
        path_to_search = os.path.join(translated_save_path, content_name)
        print(f"å·²æä¾› content_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")

    # ä½¿ç”¨æ–°æ„å»ºçš„è·¯å¾„æ¥æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    target_video_file, is_bluray_disc = _find_target_video_file(path_to_search)

    if not target_video_file:
        print("æœªèƒ½åœ¨æŒ‡å®šè·¯å¾„ä¸­æ‰¾åˆ°åˆé€‚çš„è§†é¢‘æ–‡ä»¶ï¼Œæå–å¤±è´¥ã€‚")
        return mediaInfo, False, False

    # æ£€æŸ¥æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶
    if is_bluray_disc:
        print("æ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ (BDMV/CERTIFICATE)ï¼Œå°è¯•ä½¿ç”¨ BDInfo æå–ä¿¡æ¯")
        return _extract_bdinfo(path_to_search)

    try:
        print(f"å‡†å¤‡ä½¿ç”¨ MediaInfo å·¥å…·ä» '{target_video_file}' æå–...")
        media_info_parsed = MediaInfo.parse(target_video_file,
                                            output="text",
                                            full=False)
        # å¤„ç† Complete nameï¼Œåªä¿ç•™æœ€åä¸€ä¸ª / ä¹‹åçš„å†…å®¹
        media_info_str = str(media_info_parsed)
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ Complete name è¡Œä¸­çš„å®Œæ•´è·¯å¾„ä¸ºæ–‡ä»¶å
        media_info_str = re.sub(
            r'(Complete name\s*:\s*)(.+)',
            lambda m: f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
            media_info_str)
        print("ä»æ–‡ä»¶é‡æ–°æå– MediaInfo æˆåŠŸã€‚")
        return media_info_str, True, False
    except Exception as e:
        print(f"ä»æ–‡ä»¶ '{target_video_file}' å¤„ç†æ—¶å‡ºé”™: {e}ã€‚å°†è¿”å›åŸå§‹ mediainfoã€‚")
        return mediaInfo, False, False


def _extract_bdinfo(bluray_path: str) -> str:
    """
    ä½¿ç”¨ BDInfo å·¥å…·ä»è“å…‰åŸç›˜ç›®å½•æå– BDInfo ä¿¡æ¯
    
    :param bluray_path: è“å…‰åŸç›˜ç›®å½•è·¯å¾„
    :return: BDInfo æ–‡æœ¬ä¿¡æ¯
    """
    try:
        print(f"å‡†å¤‡ä½¿ç”¨ BDInfo å·¥å…·ä» '{bluray_path}' æå– BDInfo ä¿¡æ¯...")

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(bluray_path):
            print(f"é”™è¯¯ï¼šæŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {bluray_path}")
            return "bdinfoæå–å¤±è´¥ï¼šæŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨ã€‚"

        # æ£€æŸ¥BDInfoå·¥å…·æ˜¯å¦å­˜åœ¨
        bdinfo_path = "/home/sqing/Codes/Docker.pt-nexus-dev/bdinfo/BDInfo"
        if not os.path.exists(bdinfo_path):
            print(f"é”™è¯¯ï¼šBDInfoå·¥å…·ä¸å­˜åœ¨: {bdinfo_path}")
            return "bdinfoæå–å¤±è´¥ï¼šBDInfoå·¥å…·æœªæ‰¾åˆ°ã€‚"

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨ BDInfo è¾“å‡º
        with tempfile.NamedTemporaryFile(mode='w+',
                                         suffix='.txt',
                                         delete=False) as temp_file:
            temp_filename = temp_file.name

        try:
            # æ„å»º BDInfo å‘½ä»¤
            bdinfo_cmd = [
                bdinfo_path,
                "-p",
                bluray_path,
                "-o",
                temp_filename,
                "-m"  # ç”Ÿæˆæ‘˜è¦
            ]

            print(f"æ‰§è¡Œ BDInfo å‘½ä»¤: {' '.join(bdinfo_cmd)}")

            # æ‰§è¡Œ BDInfo å‘½ä»¤
            result = subprocess.run(
                bdinfo_cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶ï¼ˆBDInfoå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
            )

            print(f"BDInfoæ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
            if result.stdout:
                print(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
            if result.stderr:
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")

            if result.returncode != 0:
                print(f"BDInfo æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return f"bdinfoæå–å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}ï¼Œé”™è¯¯: {result.stderr}"

            # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(temp_filename):
                print("BDInfo æœªåˆ›å»ºè¾“å‡ºæ–‡ä»¶")
                return "bdinfoæå–å¤±è´¥ï¼šBDInfoæœªåˆ›å»ºè¾“å‡ºæ–‡ä»¶ã€‚"

            # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
            file_mod_time = os.path.getmtime(temp_filename)
            print(f"è¾“å‡ºæ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´: {file_mod_time}")

            # è¯»å– BDInfo è¾“å‡ºæ–‡ä»¶
            with open(temp_filename, 'r', encoding='utf-8') as f:
                bdinfo_content = f.read()

            if not bdinfo_content:
                print("BDInfo è¾“å‡ºä¸ºç©º")
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(temp_filename)
                print(f"è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                return "bdinfoæå–ç»“æœä¸ºç©ºï¼Œè¯·æ‰‹åŠ¨è·å–ã€‚"

            print("BDInfo æå–æˆåŠŸ")
            return bdinfo_content

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_filename):
                os.unlink(temp_filename)

    except subprocess.TimeoutExpired:
        print("BDInfo æ‰§è¡Œè¶…æ—¶")
        return "bdinfoæå–è¶…æ—¶ï¼Œè¯·æ‰‹åŠ¨è·å–ã€‚"
    except Exception as e:
        print(f"BDInfo æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return f"bdinfoæå–å¤±è´¥: {str(e)}"

def upload_data_title(title: str, torrent_filename: str = ""):
    """
    ä»ç§å­ä¸»æ ‡é¢˜ä¸­æå–æ‰€æœ‰å‚æ•°ï¼Œå¹¶å¯é€‰åœ°ä»ç§å­æ–‡ä»¶åä¸­è¡¥å……ç¼ºå¤±å‚æ•°ã€‚
    """
    print(f"å¼€å§‹ä»ä¸»æ ‡é¢˜è§£æå‚æ•°: {title}")

    # 1. é¢„å¤„ç†
    original_title_str = title.strip()
    params = {}
    unrecognized_parts = []

    # ä¿æŒåŸå§‹æ ‡é¢˜ï¼Œè®©åç»­çš„åˆ¶ä½œç»„æå–é€»è¾‘æ¥å¤„ç†
    title = original_title_str

    title = re.sub(r"[ï¿¡â‚¬]", "", title)
    title = re.sub(r"\s*å‰©é¤˜æ™‚é–“.*$", "", title)
    title = re.sub(r"[\s\.]*(mkv|mp4)$", "", title,
                   flags=re.IGNORECASE).strip()
    title = re.sub(r"\[.*?\]|ã€.*?ã€‘", "", title).strip()
    title = title.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    title = title.replace("'", "")
    title = re.sub(r"(\d+[pi])([A-Z])", r"\1 \2", title)

    # 2. ä¼˜å…ˆæå–åˆ¶ä½œç»„ä¿¡æ¯
    release_group = ""
    main_part = title

    # æ£€æŸ¥ç‰¹æ®Šåˆ¶ä½œç»„ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
    special_groups = ["mUHD-FRDS", "MNHD-FRDS", "DMG&VCB-Studio", "VCB-Studio"]
    found_special_group = False
    for group in special_groups:
        if title.endswith(f" {group}") or title.endswith(f"-{group}"):
            release_group = group
            main_part = title[:-len(group) - 1].strip()
            found_special_group = True
            break

    # å¦‚æœä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œå…ˆå°è¯•åŒ¹é… VCB-Studio å˜ä½“
    if not found_special_group:
        vcb_variant_pattern = re.compile(
            r"^(?P<main_part>.+?)[-](?P<release_group>[\w\s]+&VCB-Studio)$",
            re.IGNORECASE)
        vcb_match = vcb_variant_pattern.match(title)
        if vcb_match:
            main_part = vcb_match.group("main_part").strip()
            release_group = vcb_match.group("release_group")
            found_special_group = True
            print(f"æ£€æµ‹åˆ° VCB-Studio å˜ä½“åˆ¶ä½œç»„: {release_group}")

    # å¦‚æœè¿˜ä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼åŒ¹é…
    if not found_special_group:
        general_regex = re.compile(
            r"^(?P<main_part>.+?)[-@](?P<release_group>[^\s]+)$",
            re.IGNORECASE,
        )
        print(f"[è°ƒè¯•] å°è¯•åŒ¹é…åˆ¶ä½œç»„ï¼Œæ ‡é¢˜: {title}")
        match = general_regex.match(title)
        if match:
            main_part = match.group("main_part").strip()
            release_group = match.group("release_group").strip()
            print(f"[è°ƒè¯•] æ­£åˆ™åŒ¹é…æˆåŠŸ!")
            print(f"[è°ƒè¯•]   - main_part: {main_part}")
            print(f"[è°ƒè¯•]   - release_group: {release_group}")
            print(f"[è°ƒè¯•] æœ€ç»ˆåˆ¶ä½œç»„: {release_group}")
        else:
            if title.upper().endswith("-NOGROUP"):
                release_group = "NOGROUP"
                main_part = title[:-8].strip()
            else:
                release_group = "N/A (æ— å‘å¸ƒç»„)"

    # 3. å­£é›†ã€å¹´ä»½ã€å‰ªè¾‘ç‰ˆæœ¬æå–
    season_match = re.search(
        r"(?<!\w)(S\d{1,2}(?:(?:[-â€“~]\s*S?\d{1,2})?|(?:\s*E\d{1,3}(?:[-â€“~]\s*(?:S\d{1,2})?E?\d{1,3})*)?))(?!\w)",
        main_part,
        re.I,
    )
    if season_match:
        season_str = season_match.group(1)
        main_part = main_part.replace(season_str, " ").strip()
        params["season_episode"] = re.sub(r"\s", "", season_str.upper())

    title_part = main_part
    year_match = re.search(r"[\s\.\(]((?:19|20)\d{2})([\s\.\)]|$)", title_part)
    if year_match:
        params["year"] = year_match.group(1)
        title_part = title_part.replace(year_match.group(0), " ", 1).strip()

    # 4.1 æå–å‰ªè¾‘ç‰ˆæœ¬å¹¶æ‹¼æ¥åˆ°å¹´ä»½
    cut_version_pattern = re.compile(
        r"(?<!\w)(Theatrical[\s\.]?Cut|Directors?[\s\.]?Cut|DC|Extended[\s\.]?(?:Cut|Edition)|Final[\s\.]?Cut|Anniversary[\s\.]?Edition|Restored|Remastered|Criterion[\s\.]?(?:Edition|Collection)|Ultimate[\s\.]?Cut|IMAX[\s\.]?Edition|Open[\s\.]?Matte|Unrated[\s\.]?Cut)(?!\w)",
        re.IGNORECASE)
    cut_version_match = cut_version_pattern.search(title_part)
    if cut_version_match:
        cut_version = re.sub(r'[\s\.]+', ' ',
                             cut_version_match.group(1).strip())
        if "year" in params:
            params["year"] = f"{params['year']} {cut_version}"
        else:
            params["year"] = cut_version
        title_part = title_part.replace(cut_version_match.group(0), " ",
                                        1).strip()
        print(f"æ£€æµ‹åˆ°å‰ªè¾‘ç‰ˆæœ¬: {cut_version}ï¼Œå·²æ‹¼æ¥åˆ°å¹´ä»½")

    # 4. é¢„å¤„ç†æ ‡é¢˜ï¼šä¿®å¤éŸ³é¢‘å‚æ•°æ ¼å¼
    title_part = re.sub(
        r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
        r"\1 \2.\3",
        title_part,
        flags=re.I)
    title_part = re.sub(
        r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
        r"\1 \2",
        title_part,
        flags=re.I)

    # æŠ€æœ¯æ ‡ç­¾æå–ï¼ˆæ’é™¤å·²è¯†åˆ«çš„åˆ¶ä½œç»„åç§°ï¼‰
    tech_patterns_definitions = {
        "medium":
        r"UHDTV|UHD\s*Blu-?ray|Blu-?ray\s+DIY|Blu-ray|BluRay\s+DIY|BluRay|BDrip|BD-?rip|WEB-DL|WEBrip|TVrip|DVDRip|HDTV",
        "audio":
        r"DTS-?HD\s*MA(?:\s*\d\.\d)?(?:\s*X)?|DTS-?HD\s*HR(?:\s*\d\.\d)?|DTS-?HD(?:\s*\d\.\d)?|DTS-?X(?:\s*\d\.\d)?|DTS\s*X(?:\s*\d\.\d)?|DTS(?:\s*\d\.\d)?|(?:Dolby\s*)?TrueHD(?:\s*Atmos)?(?:\s*\d\.\d)?|Atmos(?:\s*TrueHD)?(?:\s*\d\.\d)?|DDP\s*Atmos(?:\s*\d\.\d)?|DDP(?:\s*\d\.\d)?|E-?AC-?3(?:\s*\d\.\d)?|DD\+(?:\s*\d\.\d)?|DD(?:\s*\d\.\d)?|AC3(?:\s*\d\.\d)?|FLAC(?:\s*\d\.\d)?|AAC(?:\s*\d\.\d)?|LPCM(?:\s*/\s*PCM)?(?:\s*\d\.\d)?|PCM(?:\s*\d\.\d)?|AV3A\s*\d\.\d|\d+\s*Audios?|MP2|DUAL",
        "hdr_format":
        r"Dolby Vision|DoVi|HDR10\+|HDRVivid|HDR10|HLG|HDR|SDR|DV|Vivid",
        "resolution": r"\d{3,4}[pi]|4K",
        "video_codec":
        r"HEVC|AVC|x265|H\s*[\s\.]?\s*265|x264|H\s*[\s\.]?\s*264|VC-1|AV1|MPEG-2",
        "source_platform":
        r"Apple TV\+|ViuTV|MyTVSuper|MyVideo|AMZN|Netflix|NF|DSNP|MAX|ATVP|iTunes|friDay|USA|EUR|JPN|CEE|FRA|LINETV|EDR|PCOK|Hami|GBR|NowPlayer|CR|SEEZN|GER|CHN|MA|Viu|Baha|KKTV|IQ|HKG|ITA|ESP",
        "bit_depth": r"\b(?:8|10)bit\b",
        "framerate": r"\d{2,3}fps",
        "completion_status": r"Complete|COMPLETE",
        "video_format": r"3D|HSBS",
        "release_version": r"REMASTERED|REPACK|RERIP|PROPER|REPOST|V\d+",
        "cut_version":
        r"Theatrical[\s\.]?Cut|Directors?[\s\.]?Cut|DC|Extended[\s\.]?(?:Cut|Edition)|Final[\s\.]?Cut|Anniversary[\s\.]?Edition|Restored|Remastered|Criterion[\s\.]?(?:Edition|Collection)|Ultimate[\s\.]?Cut|IMAX[\s\.]?Edition|Open[\s\.]?Matte|Unrated[\s\.]?Cut",
        "quality_modifier": r"MAXPLUS|HQ|EXTENDED|REMUX|EE|MiniBD",
    }
    priority_order = [
        "completion_status",
        "release_version",
        "cut_version",
        "medium",
        "resolution",
        "video_codec",
        "bit_depth",
        "hdr_format",
        "video_format",
        "framerate",
        "source_platform",
        "audio",
        "quality_modifier",
    ]

    title_candidate = title_part
    first_tech_tag_pos = len(title_candidate)
    all_found_tags = []

    release_group_keywords = []
    if release_group and release_group != "N/A (æ— å‘å¸ƒç»„)":
        release_group_keywords = re.split(r'[@\-\s]+', release_group)
        release_group_keywords = [
            kw.strip() for kw in release_group_keywords if kw.strip()
        ]
        print(f"[è°ƒè¯•] åˆ¶ä½œç»„å…³é”®è¯åˆ—è¡¨: {release_group_keywords}")

    for key in priority_order:
        pattern = tech_patterns_definitions[key]
        search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                     re.IGNORECASE) if r"\b" not in pattern
                          else re.compile(pattern, re.IGNORECASE))
        matches = list(search_pattern.finditer(title_candidate))
        if not matches:
            continue

        first_tech_tag_pos = min(first_tech_tag_pos, matches[0].start())
        raw_values = [
            m.group(0).strip() if r"\b" in pattern else m.group(1).strip()
            for m in matches
        ]

        filtered_values = []
        for val in raw_values:
            is_release_group_part = any(val.upper() == kw.upper()
                                        for kw in release_group_keywords)
            if is_release_group_part:
                print(f"[è°ƒè¯•] è¿‡æ»¤æ‰åˆ¶ä½œç»„å…³é”®è¯: {val} (å±äº {key})")
            if not is_release_group_part:
                filtered_values.append(val)

        all_found_tags.extend(filtered_values)
        if filtered_values:
            print(f"[è°ƒè¯•] '{key}' å­—æ®µæå–åˆ°æŠ€æœ¯æ ‡ç­¾: {filtered_values}")
        
        raw_values = filtered_values
        
        # --- ä¿®æ”¹å¼€å§‹ï¼šç»Ÿä¸€å¤„ç†é€»è¾‘ ---
        processed_values = raw_values

        # 1. éŸ³é¢‘ç‰¹æ®Šå¤„ç†
        if key == "audio":
            processed_values = [re.sub(r"(DD)\+", r"\1+", val, flags=re.I) for val in raw_values]
            processed_values = [
                re.sub(
                    r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
                    r"\1 \2.\3",
                    val,
                    flags=re.I) for val in processed_values
            ]
            processed_values = [
                re.sub(r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                       r"\1 \2",
                       val,
                       flags=re.I) for val in processed_values
            ]
            audio_standardization_rules = [
                (r"DTS-?HD\s*MA", r"DTS-HD MA"),
                (r"DTS-?HD\s*HR", r"DTS-HD HR"),
                (r"True-?HD\s*Atmos", r"TrueHD Atmos"),
                (r"True[-\s]?HD", r"TrueHD"),
                (r"DDP\s*Atmos", r"DDP Atmos"),
            ]
            for pattern_rgx, replacement in audio_standardization_rules:
                processed_values = [
                    re.sub(pattern_rgx, replacement, val, flags=re.I)
                    for val in processed_values
                ]
        
        # 2. è§†é¢‘ç¼–ç ç‰¹æ®Šå¤„ç†ï¼ˆè¡¥å……ç¼ºå¤±çš„ç‚¹ï¼‰
        elif key == "video_codec":
            # ä¿®å¤ H 265 / H265 -> H.265
            processed_values = [
                re.sub(r"H\s*[\s\.]?\s*265", r"H.265", val, flags=re.I) 
                for val in processed_values
            ]
            # ä¿®å¤ H 264 / H264 -> H.264
            processed_values = [
                re.sub(r"H\s*[\s\.]?\s*264", r"H.264", val, flags=re.I) 
                for val in processed_values
            ]
        # --- ä¿®æ”¹ç»“æŸ ---

        unique_processed = sorted(
            list(set(processed_values)),
            key=lambda x: title_candidate.find(x.replace(" ", "")))
        if unique_processed:
            params[key] = unique_processed[0] if len(
                unique_processed) == 1 else unique_processed

    # --- [æ–°å¢] å¼€å§‹: ä»ç§å­æ–‡ä»¶åè¡¥å……ç¼ºå¤±çš„å‚æ•° ---
    if torrent_filename:
        print(f"å¼€å§‹ä»ç§å­æ–‡ä»¶åè¡¥å……å‚æ•°: {torrent_filename}")
        filename_base = re.sub(r'(\.original)?\.torrent',
                               '',
                               torrent_filename,
                               flags=re.IGNORECASE)
        filename_candidate = re.sub(r'[\._\[\]\(\)]', ' ', filename_base)

        for key in priority_order:
            if key in params and params.get(key):
                continue

            pattern = tech_patterns_definitions[key]
            search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                         re.IGNORECASE) if r"\b" not in pattern
                              else re.compile(pattern, re.IGNORECASE))

            matches = list(search_pattern.finditer(filename_candidate))
            if matches:
                raw_values = [
                    m.group(0).strip()
                    if r"\b" in pattern else m.group(1).strip()
                    for m in matches
                ]

                # --- ä¿®æ”¹å¼€å§‹ï¼šæ–‡ä»¶åè¡¥å……é€»è¾‘ä¸­ä¹Ÿæ·»åŠ  video_codec æ ‡å‡†åŒ– ---
                processed_values = raw_values

                if key == "audio":
                    processed_values = [re.sub(r"(DD)\\+", r"\1+", val, flags=re.I) for val in raw_values]
                    processed_values = [
                        re.sub(
                            r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
                            r"\1 \2.\3",
                            val,
                            flags=re.I) for val in processed_values
                    ]
                    processed_values = [
                        re.sub(
                            r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                            r"\1 \2",
                            val,
                            flags=re.I) for val in processed_values
                    ]
                    audio_standardization_rules = [
                        (r"DTS-?HD\s*MA", r"DTS-HD MA"),
                        (r"DTS-?HD\s*HR", r"DTS-HD HR"),
                        (r"DTS-?X", r"DTS:X"),
                        (r"DTS\s*X", r"DTS:X"),
                        (r"True-?HD\s*Atmos", r"TrueHD Atmos"),
                        (r"True[-\s]?HD", r"TrueHD"),
                        (r"DDP\s*Atmos", r"DDP Atmos"),
                        (r"E[-\s]?AC[-\s]?3", r"E-AC-3"),
                        (r"DD\+", r"DD+"),
                        (r"LPCM\s*/\s*PCM", r"LPCM"),
                        (r"PCM", r"PCM"),
                    ]
                    for pattern_rgx, replacement in audio_standardization_rules:
                        processed_values = [
                            re.sub(pattern_rgx, replacement, val, flags=re.I)
                            for val in processed_values
                        ]
                
                elif key == "video_codec":
                    # ä¿®å¤ H 265 / H265 -> H.265
                    processed_values = [
                        re.sub(r"H\s*[\s\.]?\s*265", r"H.265", val, flags=re.I) 
                        for val in processed_values
                    ]
                    # ä¿®å¤ H 264 / H264 -> H.264
                    processed_values = [
                        re.sub(r"H\s*[\s\.]?\s*264", r"H.264", val, flags=re.I) 
                        for val in processed_values
                    ]
                # --- ä¿®æ”¹ç»“æŸ ---

                unique_processed = sorted(
                    list(set(processed_values)),
                    key=lambda x: filename_candidate.find(x.replace(" ", "")))

                if unique_processed:
                    print(f"   [æ–‡ä»¶åè¡¥å……] æ‰¾åˆ°ç¼ºå¤±å‚æ•° '{key}': {unique_processed}")
                    params[key] = unique_processed[0] if len(
                        unique_processed) == 1 else unique_processed
                    all_found_tags.extend(unique_processed)
    # --- [æ–°å¢] ç»“æŸ ---

    # å°†åˆ¶ä½œç»„ä¿¡æ¯æ·»åŠ åˆ°æœ€åçš„å‚æ•°ä¸­
    params["release_info"] = release_group

    if "quality_modifier" in params:
        modifiers = params.pop("quality_modifier")
        if not isinstance(modifiers, list):
            modifiers = [modifiers]
        if "medium" in params:
            medium_str = (params["medium"] if isinstance(
                params["medium"], str) else params["medium"][0])
            params["medium"] = f"{medium_str} {' '.join(sorted(modifiers))}"

    # 5. æœ€ç»ˆæ ‡é¢˜å’Œæœªè¯†åˆ«å†…å®¹ç¡®å®š
    title_zone = title_part[:first_tech_tag_pos].strip()
    tech_zone = title_part[first_tech_tag_pos:].strip()
    params["title"] = re.sub(r"[\s\.]+", " ", title_zone).strip()

    print(f"[è°ƒè¯•] å¼€å§‹æ¸…ç†æŠ€æœ¯åŒºåŸŸï¼ŒåŸå§‹æŠ€æœ¯åŒº: '{tech_zone}'")
    print(f"[è°ƒè¯•] æ‰€æœ‰å·²è¯†åˆ«æ ‡ç­¾: {all_found_tags}")

    cleaned_tech_zone = tech_zone
    for tag in sorted(all_found_tags, key=len, reverse=True):
        if re.search(r'[\u4e00-\u9fa5]', tag):
            pattern_to_remove = re.escape(tag)
        else:
            pattern_to_remove = r"\b" + re.escape(tag) + r"(?!\w)"

        before = cleaned_tech_zone
        cleaned_tech_zone = re.sub(pattern_to_remove,
                                   " ",
                                   cleaned_tech_zone,
                                   flags=re.IGNORECASE)
    
    remains = re.split(r"[\s\.]+", cleaned_tech_zone)
    unrecognized_parts.extend([part for part in remains if part])
    if unrecognized_parts:
        params["unrecognized"] = " ".join(sorted(list(
            set(unrecognized_parts))))

    english_params = {}
    key_order = [
        "title",
        "year",
        "season_episode",
        "completion_status",
        "release_version",
        "resolution",
        "medium",
        "source_platform",
        "video_codec",
        "video_format",
        "hdr_format",
        "bit_depth",
        "framerate",
        "audio",
        "release_info",
        "unrecognized",
    ]
    for key in key_order:
        if key in params and params[key]:
            if key == "audio" and isinstance(params[key], list):
                processed_audio = []
                for audio_item in params[key]:
                    match = re.match(r'^(\d+)\s*(Audio[s]?)\s+(.+)$',
                                     audio_item, re.IGNORECASE)
                    if match:
                        number = match.group(1)
                        audio_word = match.group(2)
                        codec = match.group(3)
                        processed_audio.append(f"{codec} {number}{audio_word}")
                    else:
                        processed_audio.append(audio_item)

                sorted_audio = sorted(
                    processed_audio,
                    key=lambda s:
                    (bool(re.search(r'\d+\s*Audio[s]?$', s, re.IGNORECASE)),
                     -len(s)))
                english_params[key] = " ".join(sorted_audio)
            else:
                english_params[key] = params[key]

    if "source_platform" in english_params and "audio" in english_params:
        sp_value = english_params["source_platform"]
        if isinstance(sp_value, list):
            sp_value = sp_value[0] if sp_value else ""
        if sp_value == "MA" and "MA" in str(english_params["audio"]):
            del english_params["source_platform"]
        else:
            english_params["source_platform"] = sp_value

    # 6. æœ‰æ•ˆæ€§è´¨æ£€
    is_valid = bool(english_params.get("title"))
    if is_valid:
        if not any(
                key in english_params
                for key in ["resolution", "medium", "video_codec", "audio"]):
            is_valid = False
        release_info = english_params.get("release_info", "")
        if "N/A" in release_info and "NOGROUP" not in release_info:
            core_tech_keys = ["resolution", "medium", "video_codec"]
            if sum(1 for key in core_tech_keys if key in english_params) < 2:
                is_valid = False

    if not is_valid:
        print("ä¸»æ ‡é¢˜è§£æå¤±è´¥æˆ–æœªé€šè¿‡è´¨æ£€ã€‚")
        english_params = {"title": original_title_str, "unrecognized": "è§£æå¤±è´¥"}

    translation_map = {
        "title": "ä¸»æ ‡é¢˜",
        "year": "å¹´ä»½",
        "season_episode": "å­£é›†",
        "resolution": "åˆ†è¾¨ç‡",
        "medium": "åª’ä»‹",
        "source_platform": "ç‰‡æºå¹³å°",
        "video_codec": "è§†é¢‘ç¼–ç ",
        "hdr_format": "HDRæ ¼å¼",
        "bit_depth": "è‰²æ·±",
        "framerate": "å¸§ç‡",
        "audio": "éŸ³é¢‘ç¼–ç ",
        "release_info": "åˆ¶ä½œç»„",
        "completion_status": "å‰§é›†çŠ¶æ€",
        "unrecognized": "æ— æ³•è¯†åˆ«",
        "video_format": "è§†é¢‘æ ¼å¼",
        "release_version": "å‘å¸ƒç‰ˆæœ¬",
    }

    chinese_keyed_params = {}
    for key, value in english_params.items():
        chinese_key = translation_map.get(key)
        if chinese_key:
            chinese_keyed_params[chinese_key] = value

    all_possible_keys_ordered = [
        "ä¸»æ ‡é¢˜",
        "å¹´ä»½",
        "å­£é›†",
        "å‰§é›†çŠ¶æ€",
        "å‘å¸ƒç‰ˆæœ¬",
        "åˆ†è¾¨ç‡",
        "åª’ä»‹",
        "ç‰‡æºå¹³å°",
        "è§†é¢‘ç¼–ç ",
        "è§†é¢‘æ ¼å¼",
        "HDRæ ¼å¼",
        "è‰²æ·±",
        "å¸§ç‡",
        "éŸ³é¢‘ç¼–ç ",
        "åˆ¶ä½œç»„",
        "æ— æ³•è¯†åˆ«",
    ]

    final_components_list = []
    for key in all_possible_keys_ordered:
        final_components_list.append({
            "key": key,
            "value": chinese_keyed_params.get(key, "")
        })

    print(f"ä¸»æ ‡é¢˜è§£ææˆåŠŸã€‚")
    return final_components_list
def upload_data_screenshot(source_info,
                           save_path,
                           torrent_name=None,
                           downloader_id=None):
    """
    [æœ€ç»ˆHDRä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ mpv ä»è§†é¢‘æ–‡ä»¶ä¸­æˆªå–å¤šå¼ å›¾ç‰‡ï¼Œå¹¶ä¸Šä¼ åˆ°å›¾åºŠã€‚
    - æ–°å¢HDRè‰²è°ƒæ˜ å°„å‚æ•°ï¼Œç¡®ä¿HDRè§†é¢‘æˆªå›¾é¢œè‰²æ­£å¸¸ã€‚
    - æŒ‰é¡ºåºä¸€å¼ ä¸€å¼ å¤„ç†ï¼Œç®€åŒ–æµç¨‹ã€‚
    - é‡‡ç”¨æ™ºèƒ½æ—¶é—´ç‚¹åˆ†æã€‚
    """
    if Image is None:
        print("é”™è¯¯ï¼šPillow åº“æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œæˆªå›¾ä»»åŠ¡ã€‚")
        return ""

    print("å¼€å§‹æ‰§è¡Œæˆªå›¾å’Œä¸Šä¼ ä»»åŠ¡ (å¼•æ“: mpv, è¾“å‡ºæ ¼å¼: JPEG, æ¨¡å¼: é¡ºåºæ‰§è¡Œ)...")
    config = config_manager.get()
    hoster = config.get("cross_seed", {}).get("image_hoster", "pixhost")
    num_screenshots = 5
    print(f"å·²é€‰æ‹©å›¾åºŠæœåŠ¡: {hoster}, æˆªå›¾æ•°é‡: {num_screenshots}")

    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    if torrent_name:
        full_video_path = os.path.join(translated_save_path, torrent_name)
        print(f"ä½¿ç”¨å®Œæ•´è§†é¢‘è·¯å¾„: {full_video_path}")
    else:
        full_video_path = translated_save_path
        print(f"ä½¿ç”¨åŸå§‹è·¯å¾„: {full_video_path}")

    # --- ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ (æ­¤éƒ¨åˆ†ä¿æŒä¸å˜) ---
    use_proxy = False
    proxy_config = None
    if downloader_id:
        downloaders = config.get("downloaders", [])
        for downloader in downloaders:
            if downloader.get("id") == downloader_id:
                use_proxy = downloader.get("use_proxy", False)
                if use_proxy:
                    host_value = downloader.get('host', '')
                    proxy_port = downloader.get('proxy_port', 9090)
                    if host_value.startswith(('http://', 'https://')):
                        parsed_url = urlparse(host_value)
                    else:
                        parsed_url = urlparse(f"http://{host_value}")
                    proxy_ip = parsed_url.hostname
                    if not proxy_ip:
                        if '://' in host_value:
                            proxy_ip = host_value.split('://')[1].split(
                                ':')[0].split('/')[0]
                        else:
                            proxy_ip = host_value.split(':')[0]
                    proxy_config = {
                        "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                    }
                break

    if use_proxy and proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç†æˆªå›¾: {proxy_config['proxy_base_url']}")
        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/screenshot",
                json={"remote_path": full_video_path},
                timeout=300)
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("ä»£ç†æˆªå›¾ä¸Šä¼ æˆåŠŸ")
                return result.get("bbcode", "")
            else:
                print(f"ä»£ç†æˆªå›¾ä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return ""
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å–æˆªå›¾å¤±è´¥: {e}")
            return ""

    # --- æœ¬åœ°æˆªå›¾é€»è¾‘ ---
    target_video_file, is_bluray_disc = _find_target_video_file(
        full_video_path)
    if not target_video_file:
        print("é”™è¯¯ï¼šåœ¨æŒ‡å®šè·¯å¾„ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ã€‚")
        return ""

    # å¯¹äºåŸç›˜æ–‡ä»¶ï¼Œä»ç„¶è¿›è¡Œæˆªå›¾å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if is_bluray_disc:
        print("æ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ï¼Œä½†ä»å°†è¿›è¡Œæˆªå›¾å¤„ç†")

    if not shutil.which("mpv"):
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° mpvã€‚è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶å·²æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ PATH ä¸­ã€‚")
        return ""

    screenshot_points = _get_smart_screenshot_points(target_video_file,
                                                     num_screenshots)
    if len(screenshot_points) < num_screenshots:
        print("è­¦å‘Š: æ™ºèƒ½åˆ†æå¤±è´¥æˆ–å­—å¹•ä¸è¶³ï¼Œå›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”æˆªå›¾ã€‚")
        try:
            cmd_duration = [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", target_video_file
            ]
            result = subprocess.run(cmd_duration,
                                    capture_output=True,
                                    text=True,
                                    check=True,
                                    encoding='utf-8')
            duration = float(result.stdout.strip())
            screenshot_points = [
                duration * p for p in [0.15, 0.30, 0.50, 0.70, 0.85]
            ]
        except Exception as e:
            print(f"é”™è¯¯: è¿è·å–è§†é¢‘æ—¶é•¿éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æˆªå›¾ã€‚{e}")
            return ""

    auth_token = _get_agsv_auth_token() if hoster == "agsv" else None
    if hoster == "agsv" and not auth_token:
        print("âŒ æ— æ³•è·å– æœ«æ—¥å›¾åºŠ Tokenï¼Œæˆªå›¾ä¸Šä¼ ä»»åŠ¡ç»ˆæ­¢ã€‚")
        return ""

    uploaded_urls = []
    temp_files_to_cleanup = []

    for i, screenshot_time in enumerate(screenshot_points):
        print(f"\n--- å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(screenshot_points)} å¼ æˆªå›¾ ---")

        safe_name = re.sub(r'[\\/*?:"<>|\'\s\.]+', '_',
                           source_info.get('main_title', f's_{i+1}'))  # æ›´çŸ­çš„æ–‡ä»¶å
        timestamp = f"{int(time.time()) % 1000000}"  # æ›´çŸ­çš„æ—¶é—´æˆ³
        intermediate_png_path = os.path.join(
            TEMP_DIR, f"s_{i+1}_{timestamp}.png")  # æ›´çŸ­çš„æ–‡ä»¶å
        final_jpeg_path = os.path.join(TEMP_DIR,
                                       f"s_{i+1}_{timestamp}.jpg")  # æ›´çŸ­çš„æ–‡ä»¶å
        temp_files_to_cleanup.extend([intermediate_png_path, final_jpeg_path])

        # --- [æ ¸å¿ƒä¿®æ”¹] ---
        # ä¸º mpv å‘½ä»¤æ·»åŠ  HDR è‰²è°ƒæ˜ å°„å‚æ•°
        cmd_screenshot = [
            "mpv",
            "--no-audio",
            f"--start={screenshot_time:.2f}",
            "--frames=1",

            # --- HDR è‰²è°ƒæ˜ å°„å‚æ•° ---
            # æŒ‡å®šè¾“å‡ºä¸ºæ ‡å‡†çš„sRGBè‰²å½©ç©ºé—´ï¼Œè¿™æ˜¯æ‰€æœ‰SDRå›¾ç‰‡çš„åŸºç¡€
            "--target-trc=srgb",
            # ä½¿ç”¨ 'hable' ç®—æ³•è¿›è¡Œè‰²è°ƒæ˜ å°„ï¼Œå®ƒèƒ½åœ¨ä¿ç•™é«˜å…‰å’Œé˜´å½±ç»†èŠ‚æ–¹é¢å–å¾—è‰¯å¥½å¹³è¡¡
            "--tone-mapping=hable",
            # å¦‚æœè‰²å½©ä¾ç„¶ä¸å‡†ï¼Œå¯ä»¥å°è¯•æ›´ç°ä»£çš„ 'bt.2390' ç®—æ³•
            # "--tone-mapping=bt.2390",
            f"--o={intermediate_png_path}",
            target_video_file
        ]
        # --- [æ ¸å¿ƒä¿®æ”¹ç»“æŸ] ---

        try:
            subprocess.run(cmd_screenshot,
                           check=True,
                           capture_output=True,
                           timeout=180)

            if not os.path.exists(intermediate_png_path):
                print(f"âŒ é”™è¯¯: mpv å‘½ä»¤æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ {intermediate_png_path}")
                continue

            print(
                f"   -> ä¸­é—´PNGå›¾ {os.path.basename(intermediate_png_path)} ç”ŸæˆæˆåŠŸã€‚"
            )

            try:
                with Image.open(intermediate_png_path) as img:
                    rgb_img = img.convert('RGB')
                    rgb_img.save(final_jpeg_path, 'jpeg', quality=85)
                print(
                    f"   -> JPEGå‹ç¼©æˆåŠŸ (è´¨é‡: 85) -> {os.path.basename(final_jpeg_path)}"
                )
            except Exception as e:
                print(f"   âŒ é”™è¯¯: å›¾ç‰‡ä»PNGè½¬æ¢ä¸ºJPEGå¤±è´¥: {e}")
                continue

            max_retries = 3
            image_url = None
            for attempt in range(max_retries):
                print(f"   -> æ­£åœ¨ä¸Šä¼  (ç¬¬ {attempt+1}/{max_retries} æ¬¡å°è¯•)...")
                try:
                    if hoster == "agsv":
                        image_url = _upload_to_agsv(final_jpeg_path,
                                                    auth_token)
                    else:
                        image_url = _upload_to_pixhost(final_jpeg_path)
                    if image_url:
                        uploaded_urls.append(image_url)
                        break
                    else:
                        time.sleep(2)
                except Exception as e:
                    print(f"   -> ä¸Šä¼ å°è¯• {attempt+1} å‡ºç°å¼‚å¸¸: {e}")
                    time.sleep(2)

            if not image_url:
                print(f"âš ï¸  ç¬¬ {i+1} å¼ å›¾ç‰‡ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»ç„¶ä¸Šä¼ å¤±è´¥ã€‚")

        except subprocess.CalledProcessError as e:
            error_output = e.stderr.decode('utf-8', errors='ignore')
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾å¤±è´¥ã€‚")
            print(f"   -> Stderr: {error_output}")
            continue
        except subprocess.TimeoutExpired:
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾è¶…æ—¶ (è¶…è¿‡60ç§’)ã€‚")
            continue

    print("\n--- æ‰€æœ‰æˆªå›¾å¤„ç†å®Œæ¯• ---")
    print(f"æ­£åœ¨æ¸…ç†ä¸´æ—¶ç›®å½•ä¸­çš„ {len(temp_files_to_cleanup)} ä¸ªæˆªå›¾æ–‡ä»¶...")
    for item_path in temp_files_to_cleanup:
        try:
            if os.path.exists(item_path):
                os.remove(item_path)
        except OSError as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {item_path} å¤±è´¥: {e}")

    if not uploaded_urls:
        print("ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰æˆåŠŸä¸Šä¼ ä»»ä½•å›¾ç‰‡ã€‚")
        return ""

    bbcode_links = []
    for url in sorted(uploaded_urls):
        if "pixhost.to/show/" in url:
            direct_url = _convert_pixhost_url_to_direct(url)
            if direct_url:
                bbcode_links.append(f"[img]{direct_url}[/img]")
            else:
                bbcode_links.append(f"[img]{url}[/img]")
        else:
            bbcode_links.append(f"[img]{url}[/img]")

    screenshots = "\n".join(bbcode_links)
    print("æ‰€æœ‰æˆªå›¾å·²æˆåŠŸä¸Šä¼ å¹¶å·²æ ¼å¼åŒ–ä¸ºBBCodeã€‚")
    return screenshots


def add_torrent_to_downloader(detail_page_url: str,
                              save_path: str,
                              downloader_id: str,
                              db_manager,
                              config_manager,
                              direct_download_url: str = ""):
    """
    ä»ç§å­è¯¦æƒ…é¡µä¸‹è½½ .torrent æ–‡ä»¶å¹¶æ·»åŠ åˆ°æŒ‡å®šçš„ä¸‹è½½å™¨ã€‚
    [æœ€ç»ˆä¿®å¤ç‰ˆ] ä¿®æ­£äº†å‘ Transmission å‘é€æ•°æ®æ—¶çš„åŒé‡ç¼–ç é—®é¢˜ã€‚
    """
    logging.info(
        f"å¼€å§‹è‡ªåŠ¨æ·»åŠ ä»»åŠ¡: URL='{detail_page_url}', Path='{save_path}', DownloaderID='{downloader_id}'"
    )

    # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼Œå¦‚æœè®¾ç½®ä¸ºfalseåˆ™è·³è¿‡ç§å­ä¸‹è½½å’Œæ·»åŠ 
    if os.getenv("ADD_DOWNLOADS_TORRENTS") == "false":
        msg = f"æ¨¡æ‹ŸæˆåŠŸ: ç¯å¢ƒå˜é‡ADD_DOWNLOADS_TORRENTS=falseï¼Œè·³è¿‡ç§å­ä¸‹è½½å’Œæ·»åŠ "
        logging.info(msg)
        return True, msg

    # 1. æŸ¥æ‰¾å¯¹åº”çš„ç«™ç‚¹é…ç½®
    conn = db_manager._get_connection()
    cursor = db_manager._get_cursor(conn)
    cursor.execute("SELECT nickname, base_url, cookie, speed_limit FROM sites")
    site_info = None
    for site in cursor.fetchall():
        # [ä¿®å¤] ç¡®ä¿ base_url å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if site['base_url'] and site['base_url'] in detail_page_url:
            site_info = dict(site)  # [ä¿®å¤] å°† sqlite3.Row è½¬æ¢ä¸º dict
            break
    conn.close()

    if not site_info or not site_info.get("cookie"):
        msg = f"æœªèƒ½æ‰¾åˆ°ä¸URL '{detail_page_url}' åŒ¹é…çš„ç«™ç‚¹é…ç½®æˆ–è¯¥ç«™ç‚¹ç¼ºå°‘Cookieã€‚"
        logging.error(msg)
        return False, msg

    try:
        # 2. ä¸‹è½½ç§å­æ–‡ä»¶
        common_headers = {
            "Cookie":
            site_info["cookie"],
            "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        }
        scraper = cloudscraper.create_scraper()

        # ç«™ç‚¹çº§åˆ«çš„ä»£ç†å·²ä¸ä½¿ç”¨å…¨å±€ä»£ç†é…ç½®
        proxies = None
        torrent_content = None

        # å¦‚æœæä¾›äº†ç›´æ¥ä¸‹è½½é“¾æ¥ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¸‹è½½ï¼Œé¿å…è¯·æ±‚è¯¦æƒ…é¡µ
        if direct_download_url:
            try:
                logging.info(f"ä½¿ç”¨ç›´æ¥ä¸‹è½½é“¾æ¥: {direct_download_url}")

                # ä½¿ç”¨ç›´æ¥ä¸‹è½½é“¾æ¥ä¸‹è½½ç§å­æ–‡ä»¶
                direct_headers = common_headers.copy()
                scraper = cloudscraper.create_scraper()

                # Add retry logic for direct torrent download
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        torrent_response = scraper.get(direct_download_url,
                                                       headers=direct_headers,
                                                       timeout=180,
                                                       proxies=proxies)
                        torrent_response.raise_for_status()
                        break  # Success, exit retry loop
                    except Exception as e:
                        if attempt < max_retries - 1:
                            logging.warning(
                                f"Attempt {attempt + 1} failed to download torrent directly: {e}. Retrying..."
                            )
                            time.sleep(2**attempt)  # Exponential backoff
                        else:
                            raise  # Re-raise the exception if all retries failed

                torrent_content = torrent_response.content
                logging.info("å·²é€šè¿‡ç›´æ¥ä¸‹è½½é“¾æ¥æˆåŠŸä¸‹è½½ç§å­æ–‡ä»¶å†…å®¹ã€‚")

            except Exception as e:
                msg = f"ä½¿ç”¨ç›´æ¥ä¸‹è½½é“¾æ¥ä¸‹è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}"
                logging.warning(msg)
                # å¦‚æœç›´æ¥ä¸‹è½½å¤±è´¥ï¼Œç»§ç»­èµ°è¯¦æƒ…é¡µé€»è¾‘

        # å¦‚æœæ²¡æœ‰ç›´æ¥ä¸‹è½½é“¾æ¥æˆ–ç›´æ¥ä¸‹è½½å¤±è´¥ï¼Œåˆ™è¯·æ±‚è¯¦æƒ…é¡µ
        if not torrent_content:
            logging.info("æœªæä¾›ç›´æ¥ä¸‹è½½é“¾æ¥æˆ–ç›´æ¥ä¸‹è½½å¤±è´¥ï¼Œå¼€å§‹è¯·æ±‚è¯¦æƒ…é¡µ")

            # Add retry logic for network requests
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    details_response = scraper.get(detail_page_url,
                                                   headers=common_headers,
                                                   timeout=180,
                                                   proxies=proxies)
                    break  # Success, exit retry loop
                except Exception as e:
                    if attempt < max_retries - 1:
                        logging.warning(
                            f"Attempt {attempt + 1} failed to fetch details page: {e}. Retrying..."
                        )
                        time.sleep(2**attempt)  # Exponential backoff
                    else:
                        raise  # Re-raise the exception if all retries failed
            details_response.raise_for_status()

            soup = BeautifulSoup(details_response.text, "html.parser")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ç‰¹æ®Šä¸‹è½½å™¨
            site_base_url = ensure_scheme(site_info['base_url'])
            full_download_url = None  # åˆå§‹åŒ–full_download_url

            print(f"ç«™ç‚¹åŸºç¡€URL: {site_base_url}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºhaidanç«™ç‚¹
            if 'haidan' in site_base_url:
                # Haidanç«™ç‚¹éœ€è¦æå–torrent_idè€Œä¸æ˜¯id
                torrent_id_match = re.search(r"torrent_id=(\d+)",
                                             detail_page_url)
                if not torrent_id_match:
                    raise ValueError("æ— æ³•ä»è¯¦æƒ…é¡µURLä¸­æå–ç§å­IDï¼ˆtorrent_idï¼‰ã€‚")
                torrent_id = torrent_id_match.group(1)
                # Haidanç«™ç‚¹çš„ç‰¹æ®Šé€»è¾‘
                download_link_tag = soup.find(
                    'a', href=re.compile(r"download.php\?id="))

                if not download_link_tag:
                    raise RuntimeError("åœ¨è¯¦æƒ…é¡µHTMLä¸­æœªèƒ½æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼")

                download_url_part = str(download_link_tag['href'])  # æ˜¾å¼è½¬æ¢ä¸ºstr

                # æ›¿æ¢ä¸‹è½½é“¾æ¥ä¸­çš„idä¸ºä»detail_page_urlä¸­æå–çš„torrent_id
                download_url_part = re.sub(r"id=\d+", f"id={torrent_id}",
                                           download_url_part)

                full_download_url = f"{site_base_url}/{download_url_part}"
            else:
                # å…¶ä»–ç«™ç‚¹çš„é€šç”¨é€»è¾‘ - æå–idå‚æ•°
                torrent_id_match = re.search(r"id=(\d+)", detail_page_url)
                if not torrent_id_match: raise ValueError("æ— æ³•ä»è¯¦æƒ…é¡µURLä¸­æå–ç§å­IDã€‚")
                torrent_id = torrent_id_match.group(1)

                download_link_tag = soup.select_one(
                    f'a.index[href^="download.php?id={torrent_id}"]')
                if not download_link_tag:
                    raise RuntimeError("åœ¨è¯¦æƒ…é¡µHTMLä¸­æœªèƒ½æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼")

                download_url_part = str(download_link_tag['href'])  # æ˜¾å¼è½¬æ¢ä¸ºstr
                full_download_url = f"{site_base_url}/{download_url_part}"

            # ç¡®ä¿full_download_urlå·²è¢«èµ‹å€¼
            if not full_download_url:
                raise RuntimeError("æœªèƒ½æˆåŠŸæ„å»ºç§å­ä¸‹è½½é“¾æ¥ï¼")

            print(f"ç§å­ä¸‹è½½é“¾æ¥: {full_download_url}")

            common_headers["Referer"] = detail_page_url
            # Add retry logic for torrent download
            for attempt in range(max_retries):
                try:
                    torrent_response = scraper.get(full_download_url,
                                                   headers=common_headers,
                                                   timeout=180,
                                                   proxies=proxies)
                    torrent_response.raise_for_status()
                    break  # Success, exit retry loop
                except Exception as e:
                    if attempt < max_retries - 1:
                        logging.warning(
                            f"Attempt {attempt + 1} failed to download torrent: {e}. Retrying..."
                        )
                        time.sleep(2**attempt)  # Exponential backoff
                    else:
                        raise  # Re-raise the exception if all retries failed

            torrent_content = torrent_response.content
            logging.info("å·²é€šè¿‡è¯¦æƒ…é¡µæˆåŠŸä¸‹è½½ç§å­æ–‡ä»¶å†…å®¹ã€‚")

    except Exception as e:
        msg = f"åœ¨ä¸‹è½½ç§å­æ–‡ä»¶æ­¥éª¤å‘ç”Ÿé”™è¯¯: {e}"
        logging.error(msg, exc_info=True)
        return False, msg

    # 3. æ‰¾åˆ°ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloader_config = next(
        (d for d in config.get("downloaders", [])
         if d.get("id") == downloader_id and d.get("enabled")), None)

    if not downloader_config:
        msg = f"æœªæ‰¾åˆ°IDä¸º '{downloader_id}' çš„å·²å¯ç”¨ä¸‹è½½å™¨é…ç½®ã€‚"
        logging.error(msg)
        return False, msg

    # 4. æ·»åŠ åˆ°ä¸‹è½½å™¨ (æ ¸å¿ƒä¿®æ”¹åœ¨æ­¤ï¼) - æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    for attempt in range(max_retries):
        try:
            from core.services import _prepare_api_config

            api_config = _prepare_api_config(downloader_config)
            client_name = downloader_config['name']

            if downloader_config['type'] == 'qbittorrent':
                client = qbClient(**api_config)
                client.auth_log_in()

                # å‡†å¤‡ qBittorrent å‚æ•°
                qb_params = {
                    'torrent_files': torrent_content,
                    'save_path': save_path,
                    'is_paused': False,
                    'skip_checking': True
                }

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™æ·»åŠ é€Ÿåº¦é™åˆ¶å‚æ•°
                # æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯MB/sï¼Œéœ€è¦è½¬æ¢ä¸ºbytes/sä¼ é€’ç»™ä¸‹è½½å™¨API
                if site_info and site_info.get('speed_limit', 0) > 0:
                    speed_limit = int(
                        site_info['speed_limit']) * 1024 * 1024  # è½¬æ¢ä¸º bytes/s
                    qb_params['upload_limit'] = speed_limit
                    logging.info(
                        f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s"
                    )

                result = client.torrents_add(**qb_params)
                logging.info(f"å·²å°†ç§å­æ·»åŠ åˆ° qBittorrent '{client_name}': {result}")

            elif downloader_config['type'] == 'transmission':
                client = TrClient(**api_config)

                # å‡†å¤‡ Transmission å‚æ•°
                tr_params = {
                    'torrent': torrent_content,
                    'download_dir': save_path,
                    'paused': False
                }

                # å…ˆæ·»åŠ ç§å­
                result = client.add_torrent(**tr_params)
                logging.info(
                    f"å·²å°†ç§å­æ·»åŠ åˆ° Transmission '{client_name}': ID={result.id}")

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™åœ¨æ·»åŠ åè®¾ç½®é€Ÿåº¦é™åˆ¶
                # add_torrent æ–¹æ³•ä¸æ”¯æŒé€Ÿåº¦é™åˆ¶å‚æ•°ï¼Œéœ€è¦ä½¿ç”¨ change_torrent æ–¹æ³•
                if site_info and site_info.get('speed_limit', 0) > 0:
                    # è½¬æ¢ä¸º KBps: MB/s * 1024 = KBps
                    speed_limit_kbps = int(site_info['speed_limit']) * 1024
                    try:
                        client.change_torrent(result.id,
                                              upload_limit=speed_limit_kbps,
                                              upload_limited=True)
                        logging.info(
                            f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s ({speed_limit_kbps} KBps)"
                        )
                    except Exception as e:
                        logging.warning(f"è®¾ç½®é€Ÿåº¦é™åˆ¶å¤±è´¥ï¼Œä½†ç§å­å·²æ·»åŠ æˆåŠŸ: {e}")

            return True, f"æˆåŠŸæ·»åŠ åˆ° '{client_name}'"

        except Exception as e:
            logging.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ·»åŠ ç§å­åˆ°ä¸‹è½½å™¨å¤±è´¥: {e}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                logging.info(f"ç­‰å¾… {wait_time} ç§’åè¿›è¡Œç¬¬ {attempt + 2} æ¬¡å°è¯•...")
                time.sleep(wait_time)
            else:
                msg = f"æ·»åŠ åˆ°ä¸‹è½½å™¨ '{downloader_config['name']}' æ—¶å¤±è´¥: {e}"
                logging.error(msg, exc_info=True)
                return False, msg


def extract_tags_from_title(title_components: list) -> list:
    """
    ä»æ ‡é¢˜å‚æ•°ä¸­æå–æ ‡ç­¾ï¼Œä¸»è¦ä»åª’ä»‹å’Œåˆ¶ä½œç»„å­—æ®µæå– DIY å’Œ VCB-Studio æ ‡ç­¾ã€‚
    
    è¿”å›åŸå§‹æ ‡ç­¾åç§°ï¼ˆå¦‚ "DIY", "VCB-Studio"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ã€‚
    è¿™æ ·å¯ä»¥è¢« ParameterMapper æ­£ç¡®æ˜ å°„åˆ° global_mappings.yaml ä¸­å®šä¹‰çš„æ ‡å‡†åŒ–é”®ã€‚

    :param title_components: æ ‡é¢˜ç»„ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"key": "ä¸»æ ‡é¢˜", "value": "..."}, ...]
    :return: ä¸€ä¸ªåŒ…å«åŸå§‹æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['DIY', 'VCB-Studio']
    """
    if not title_components:
        return []

    found_tags = set()

    # å°† title_components è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æŸ¥æ‰¾
    title_dict = {
        item.get('key'): item.get('value', '')
        for item in title_components
    }

    # å®šä¹‰éœ€è¦æ£€æŸ¥çš„å­—æ®µå’Œå¯¹åº”çš„æ ‡ç­¾æ˜ å°„
    # æ ¼å¼ï¼šå­—æ®µå -> [(æ­£åˆ™æ¨¡å¼, åŸå§‹æ ‡ç­¾å), ...]
    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯åŸå§‹æ ‡ç­¾åï¼ˆå¦‚ "DIY"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ï¼ˆå¦‚ "tag.diy"ï¼‰
    tag_extraction_rules = {
        'åª’ä»‹': [
            (r'\bDIY\b', 'DIY'),
            (r'\bBlu-?ray\s+DIY\b', 'DIY'),
            (r'\bBluRay\s+DIY\b', 'DIY'),
            (r'\bRemux\b', 'Remux'),
        ],
        'åˆ¶ä½œç»„': [
            (r'\bDIY\b', 'DIY'),
            (r'\bVCB-Studio\b', 'VCB-Studio'),
            (r'\bVCB\b', 'VCB-Studio'),
        ]
    }

    # éå†éœ€è¦æ£€æŸ¥çš„å­—æ®µ
    for field_name, patterns in tag_extraction_rules.items():
        field_value = title_dict.get(field_name, '')

        if not field_value:
            continue

        # å¦‚æœå­—æ®µå€¼æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(field_value, list):
            field_value = ' '.join(str(v) for v in field_value)
        else:
            field_value = str(field_value)

        # æ£€æŸ¥æ¯ä¸ªæ­£åˆ™æ¨¡å¼
        for pattern, tag_name in patterns:
            if re.search(pattern, field_value, re.IGNORECASE):
                found_tags.add(tag_name)
                print(
                    f"ä»æ ‡é¢˜å‚æ•° '{field_name}' ä¸­æå–åˆ°æ ‡ç­¾: {tag_name} (åŒ¹é…: {pattern})")

    result_tags = list(found_tags)
    if result_tags:
        print(f"ä»æ ‡é¢˜å‚æ•°ä¸­æå–åˆ°çš„æ ‡ç­¾: {result_tags}")
    else:
        print("ä»æ ‡é¢˜å‚æ•°ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return result_tags


def extract_tags_from_subtitle(subtitle: str) -> list:
    """
    ä»å‰¯æ ‡é¢˜ä¸­æå–è¯­è¨€ã€å­—å¹•å’Œç‰¹æ•ˆæ ‡ç­¾ã€‚
    æ”¯æŒçš„æ ‡ç­¾ï¼šä¸­å­—ã€ç²¤è¯­ã€å›½è¯­ã€å°é…ã€ç‰¹æ•ˆ
    
    :param subtitle: å‰¯æ ‡é¢˜æ–‡æœ¬
    :return: æ ‡ç­¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.ä¸­å­—', 'tag.ç²¤è¯­', 'tag.ç‰¹æ•ˆ']
    """
    if not subtitle:
        return []

    found_tags = set()

    # é¦–å…ˆæ£€æŸ¥"ç‰¹æ•ˆ"å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œç‹¬ç«‹æ£€æµ‹ï¼‰
    if "ç‰¹æ•ˆ" in subtitle:
        found_tags.add("ç‰¹æ•ˆ")
        print(f"ä»å‰¯æ ‡é¢˜ä¸­æå–åˆ°æ ‡ç­¾: ç‰¹æ•ˆ")

    # å®šä¹‰åˆ†éš”ç¬¦ï¼Œç”¨äºæ‹†åˆ†å‰¯æ ‡é¢˜
    # æ”¯æŒï¼š[]ã€ã€ã€‘ã€|ã€*ã€/ç­‰ç¬¦å·
    # æ³¨æ„ï¼šå¯¹äº"| å†…å°å®˜è¯‘ç®€ç¹"è¿™ç§åªæœ‰å·¦è¾¹æœ‰|çš„æƒ…å†µï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    delimiter_pattern = r'[\[\]ã€ã€‘\|\*\/]'

    # é¦–å…ˆå¤„ç†ç‰¹æ®Šçš„"|"åˆ†éš”ç¬¦æƒ…å†µ
    # ä¾‹å¦‚ï¼š"| å†…å°å®˜è¯‘ç®€ç¹+ç®€è‹±ç¹è‹±åŒè¯­å­—å¹•" æˆ– "| æ±‰è¯­æ™®é€šè¯"
    special_pipe_parts = []
    if '|' in subtitle:
        # æŒ‰|åˆ†å‰²ï¼Œä¿ç•™å·¦è¾¹|çš„å†…å®¹ä½œä¸ºç‹¬ç«‹éƒ¨åˆ†
        pipe_parts = subtitle.split('|')
        for part in pipe_parts:
            if part.strip():
                special_pipe_parts.append(part.strip())

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å‰¯æ ‡é¢˜
    parts = re.split(delimiter_pattern, subtitle)

    # åˆå¹¶ä¸¤ç§åˆ†å‰²æ–¹å¼çš„ç»“æœ
    all_parts = list(set(parts + special_pipe_parts))

    # å®šä¹‰å…³é”®è¯åˆ°æ ‡ç­¾çš„æ˜ å°„
    tag_patterns = {
        'ä¸­å­—': [
            r'ä¸­[å­—å¹•]', r'ç®€[ä½“ä¸­ç¹]', r'ç¹[ä½“ä¸­ç®€]', r'ä¸­è‹±', r'ç®€è‹±', r'ç¹è‹±', r'ç®€ç¹', r'ä¸­æ—¥',
            r'ç®€æ—¥', r'ç¹æ—¥', r'å®˜è¯‘', r'å†…å°.*[ç®€ç¹]', r'[ç®€ç¹].*å­—å¹•', r'åŒè¯­å­—å¹•', r'å¤šå›½.*å­—å¹•',
            r'è½¯å­—å¹•'
        ],
        'ç²¤è¯­': [
            r'ç²¤[è¯­é…]',
            r'ç²¤éŸ³',
            r'ç²¤.*é…éŸ³',
            r'æ¸¯ç‰ˆ',
            r'æ¸¯.*é…éŸ³',
            r'\bç²¤\b'  # åŒ¹é…ç‹¬ç«‹çš„"ç²¤"å­—ï¼Œå¦‚"é™†/æ—¥/å°/ç²¤/é—½äº”è¯­"ä¸­çš„"ç²¤"
        ],
        'å›½è¯­': [
            r'å›½[è¯­é…]',
            r'å›½.*é…éŸ³',
            r'æ±‰è¯­',
            r'æ™®é€šè¯',
            r'ä¸­æ–‡é…éŸ³',
            r'åè¯­',
            r'å°é…å›½è¯­',  # ç‰¹æ®Šå¤„ç†ï¼šå°é…å›½è¯­ä¼šåŒ¹é…å›½è¯­ï¼Œä½†åç»­ä¼šè¢«å°é…è¦†ç›–
            r'\bé™†\b',
            r'\bå›½\b'  # åŒ¹é…ç‹¬ç«‹çš„"é™†"æˆ–"å›½"å­—ï¼Œå¦‚"é™†/æ—¥/å°/ç²¤/é—½äº”è¯­"ä¸­çš„"é™†"
        ],
        'å°é…': [
            r'å°[é…éŸ³]',
            r'å°.*é…éŸ³',
            r'ä¸œæ£®',
            r'çº¬æ¥',
            r'å°é…å›½è¯­',
            r'å°é….*å›½è¯­',
            r'\bå°\b'  # åŒ¹é…ç‹¬ç«‹çš„"å°"å­—ï¼Œå¦‚"é™†/æ—¥/å°/ç²¤/é—½äº”è¯­"ä¸­çš„"å°"
        ]
    }

    # éå†æ¯ä¸ªåˆ†å‰²åçš„éƒ¨åˆ†è¿›è¡Œå…³é”®è¯åŒ¹é…
    for part in all_parts:
        if not part.strip():
            continue

        part_clean = part.strip()

        # æ£€æŸ¥æ¯ä¸ªæ ‡ç­¾çš„æ¨¡å¼
        for tag_name, patterns in tag_patterns.items():
            for pattern in patterns:
                if re.search(pattern, part_clean, re.IGNORECASE):
                    found_tags.add(tag_name)
                    print(
                        f"ä»å‰¯æ ‡é¢˜æ®µè½ '{part_clean}' ä¸­æå–åˆ°æ ‡ç­¾: {tag_name} (åŒ¹é…: {pattern})"
                    )
                    # æ‰¾åˆ°åŒ¹é…åè·³å‡ºå½“å‰æ ‡ç­¾çš„æ¨¡å¼å¾ªç¯
                    break

    # ä¸ºæ‰€æœ‰æ ‡ç­¾æ·»åŠ  tag. å‰ç¼€
    prefixed_tags = [f'tag.{tag}' for tag in found_tags]

    if prefixed_tags:
        print(f"ä»å‰¯æ ‡é¢˜ä¸­æå–åˆ°çš„æ ‡ç­¾: {prefixed_tags}")
    else:
        print("ä»å‰¯æ ‡é¢˜ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return prefixed_tags


def extract_tags_from_description(description_text: str) -> list:
    """
    ä»ç®€ä»‹æ–‡æœ¬çš„"ç±»åˆ«"å­—æ®µä¸­æå–æ ‡ç­¾ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: æ ‡ç­¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å–œå‰§', 'tag.åŠ¨ç”»']
    """
    if not description_text:
        return []

    found_tags = []

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"[â—â]\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)",
                               description_text)
    if category_match:
        category_text = category_match.group(1).strip()
        print(f"ä»ç®€ä»‹ä¸­æå–åˆ°ç±»åˆ«: {category_text}")

        # å®šä¹‰ç±»åˆ«å…³é”®è¯åˆ°æ ‡ç­¾çš„æ˜ å°„
        category_tag_map = {
            'å–œå‰§': 'tag.å–œå‰§',
            'Comedy': 'tag.å–œå‰§',
            'å„¿ç«¥': 'tag.å„¿ç«¥',
            'Children': 'tag.å„¿ç«¥',
            'åŠ¨ç”»': 'tag.åŠ¨ç”»',
            'Animation': 'tag.åŠ¨ç”»',
            'åŠ¨ä½œ': 'tag.åŠ¨ä½œ',
            'Action': 'tag.åŠ¨ä½œ',
            'çˆ±æƒ…': 'tag.çˆ±æƒ…',
            'Romance': 'tag.çˆ±æƒ…',
            'ç§‘å¹»': 'tag.ç§‘å¹»',
            'Sci-Fi': 'tag.ç§‘å¹»',
            'ææ€–': 'tag.ææ€–',
            'Horror': 'tag.ææ€–',
            'æƒŠæ‚š': 'tag.æƒŠæ‚š',
            'Thriller': 'tag.æƒŠæ‚š',
            'æ‚¬ç–‘': 'tag.æ‚¬ç–‘',
            'Mystery': 'tag.æ‚¬ç–‘',
            'çŠ¯ç½ª': 'tag.çŠ¯ç½ª',
            'Crime': 'tag.çŠ¯ç½ª',
            'æˆ˜äº‰': 'tag.æˆ˜äº‰',
            'War': 'tag.æˆ˜äº‰',
            'å†’é™©': 'tag.å†’é™©',
            'Adventure': 'tag.å†’é™©',
            'å¥‡å¹»': 'tag.å¥‡å¹»',
            'Fantasy': 'tag.å¥‡å¹»',
            'å®¶åº­': 'tag.å®¶åº­',
            'Family': 'tag.å®¶åº­',
            'å‰§æƒ…': 'tag.å‰§æƒ…',
            'Drama': 'tag.å‰§æƒ…',
        }

        # æ£€æŸ¥ç±»åˆ«æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword, tag in category_tag_map.items():
            if keyword in category_text:
                found_tags.append(tag)
                print(f"   ä»ç±»åˆ«ä¸­æå–åˆ°æ ‡ç­¾: {tag} (åŒ¹é…å…³é”®è¯: {keyword})")

    if found_tags:
        print(f"ä»ç®€ä»‹ç±»åˆ«ä¸­æå–åˆ°çš„æ ‡ç­¾: {found_tags}")
    else:
        print("ä»ç®€ä»‹ç±»åˆ«ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return found_tags


def check_animation_type_from_description(description_text: str) -> bool:
    """
    æ£€æŸ¥ç®€ä»‹çš„ç±»åˆ«å­—æ®µä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ­£ç±»å‹ä¸ºåŠ¨æ¼«ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: å¦‚æœåŒ…å«"åŠ¨ç”»"è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not description_text:
        return False

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"â—\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)", description_text)
    if category_match:
        category_text = category_match.group(1).strip()

        # æ£€æŸ¥ç±»åˆ«ä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"å…³é”®è¯
        if "åŠ¨ç”»" in category_text or "Animation" in category_text:
            print(f"æ£€æµ‹åˆ°ç±»åˆ«ä¸­åŒ…å«'åŠ¨ç”»': {category_text}")
            return True

    return False


def extract_tags_from_mediainfo(mediainfo_text: str) -> list:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ ‡ç­¾åˆ—è¡¨ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: ä¸€ä¸ªåŒ…å«è¯†åˆ«å‡ºçš„æ ‡ç­¾å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å›½è¯­', 'tag.ä¸­å­—', 'tag.HDR10']ã€‚
    """
    if not mediainfo_text:
        return []

    found_tags = set()
    lines = mediainfo_text.split('\n')  # ä¸è½¬å°å†™ï¼Œä¿æŒåŸå§‹å¤§å°å†™

    # å®šä¹‰å…³é”®è¯åˆ°æ ‡å‡†åŒ–æ ‡ç­¾çš„æ˜ å°„
    tag_keywords_map = {
        # è¯­è¨€æ ‡ç­¾
        'å›½è¯­': ['å›½è¯­', 'mandarin'],
        'ç²¤è¯­': ['ç²¤è¯­', 'cantonese'],
        'è‹±è¯­': ['è‹±è¯­', 'english'],
        'æ—¥è¯­': ['æ—¥è¯­', 'Japanese', 'japanese'],
        'éŸ©è¯­': ['éŸ©è¯­', 'korean'],
        'æ³•è¯­': ['æ³•è¯­', 'french'],
        'å¾·è¯­': ['å¾·è¯­', 'german'],
        'ä¿„è¯­': ['ä¿„è¯­', 'russian'],
        'å°åœ°è¯­': ['å°åœ°è¯­', 'hindi'],
        'è¥¿ç­ç‰™è¯­': ['è¥¿ç­ç‰™è¯­', 'spanish'],
        'è‘¡è„ç‰™è¯­': ['è‘¡è„ç‰™è¯­', 'portuguese'],
        'æ„å¤§åˆ©è¯­': ['æ„å¤§åˆ©è¯­', 'italian'],
        'æ³°è¯­': ['æ³°è¯­', 'thai'],
        'é˜¿æ‹‰ä¼¯è¯­': ['é˜¿æ‹‰ä¼¯è¯­', 'arabic'],
        'å¤–è¯­': ['å¤–è¯­', 'foreign'],
        # å­—å¹•æ ‡ç­¾
        'ä¸­å­—': ['ä¸­å­—', 'chinese', 'ç®€', 'ç¹'],
        'è‹±å­—': ['è‹±å­—', 'english'],
        # HDR æ ¼å¼æ ‡ç­¾
        'Dolby Vision': ['dolby vision', 'æœæ¯”è§†ç•Œ'],
        'HDR10+': ['hdr10+'],
        'HDR10': ['hdr10'],
        'HDR': ['hdr'],  # ä½œä¸ºé€šç”¨ HDR çš„å¤‡ç”¨é€‰é¡¹
        'HDRVivid': ['hdr vivid'],
    }

    # å®šä¹‰æ£€æŸ¥èŒƒå›´
    # current_section ç”¨äºè®°å½•å½“å‰ MediaInfo æ­£åœ¨å¤„ç†çš„ Section ç±»å‹ (General, Video, Audio, Text)
    current_section = None
    # ç”¨äºæ”¶é›†å½“å‰ Audio Section çš„æ‰€æœ‰è¡Œï¼Œä»¥ä¾¿åç»­è¯­è¨€æ£€æµ‹
    current_audio_section_lines = []
    # ç”¨äºæ”¶é›†å½“å‰ Video Section çš„æ‰€æœ‰è¡Œï¼Œä»¥ä¾¿åç»­è¯­è¨€æ£€æµ‹
    current_video_section_lines = []

    for line in lines:
        line_stripped = line.strip()
        line_lower = line_stripped.lower()

        # åˆ¤å®šå½“å‰å¤„äºå“ªä¸ªä¿¡æ¯å—
        if line_lower.startswith('general'):
            current_section = 'general'
            # åœ¨ General Section ç»“æŸæ—¶å¤„ç†ä¹‹å‰çš„ Audio/Video Section
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            continue
        elif line_lower.startswith('video'):
            current_section = 'video'
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            current_video_section_lines = [line_stripped]  # å¼€å§‹æ–°çš„ Video å—
            continue
        elif line_lower.startswith('audio'):
            # å…ˆå¤„ç†ä¹‹å‰çš„ Audio å—
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
            # å¤„ç†ä¹‹å‰çš„ Video å—
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            # å¼€å§‹æ–°çš„ Audio å—
            current_section = 'audio'
            current_audio_section_lines = [line_stripped]
            continue
        elif line_lower.startswith('text'):
            current_section = 'text'
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            continue
        # å…¶ä»– Section æš‚ä¸å¤„ç†ï¼Œç›´æ¥è·³è¿‡æˆ–è€…å¯ä»¥å®šä¹‰ä¸º 'other'
        elif not line_stripped:  # ç©ºè¡Œè¡¨ç¤ºä¸€ä¸ªSectionçš„ç»“æŸï¼Œå¯ä»¥è§¦å‘å¤„ç†
            if current_audio_section_lines and current_section != 'audio':  # å¦‚æœæ˜¯ç©ºè¡Œä¸”ä¹‹å‰æ˜¯éŸ³é¢‘å—ï¼Œåˆ™å¤„ç†
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines and current_section != 'video':  # å¦‚æœæ˜¯ç©ºè¡Œä¸”ä¹‹å‰æ˜¯è§†é¢‘å—ï¼Œåˆ™å¤„ç†
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            current_section = None  # é‡ç½®å½“å‰section
            continue

        # æ”¶é›†å½“å‰ Section çš„è¡Œ
        if current_section == 'audio':
            current_audio_section_lines.append(line_stripped)
        elif current_section == 'video':
            current_video_section_lines.append(line_stripped)
        elif current_section == 'text':
            # ä»…åœ¨ Text å—ä¸­æ£€æŸ¥å­—å¹•æ ‡ç­¾
            if 'ä¸­å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['ä¸­å­—']):
                found_tags.add('ä¸­å­—')
            if 'è‹±å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['è‹±å­—']):
                found_tags.add('è‹±å­—')

        # æ£€æŸ¥ HDR æ ¼å¼æ ‡ç­¾ (å…¨å±€æ£€æŸ¥)
        # æ³¨æ„ï¼šè¿™é‡Œä¿æŒå…¨å±€æ£€æŸ¥æ˜¯å› ä¸º HDR æ ¼å¼å¯èƒ½å‡ºç°åœ¨ General/Video ç­‰å¤šä¸ªåœ°æ–¹
        if 'dolby vision' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['Dolby Vision']):
            found_tags.add('Dolby Vision')
        if 'hdr10+' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10+']):
            found_tags.add('HDR10+')
        if 'hdr10' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10']):
            found_tags.add('HDR10')
        elif 'hdr' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR']):
            if not any(hdr_tag in found_tags
                       for hdr_tag in ['Dolby Vision', 'HDR10+', 'HDR10']):
                found_tags.add('HDR')
        if 'hdrvivid' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDRVivid']):
            found_tags.add('HDRVivid')

    # å¤„ç†æ–‡ä»¶æœ«å°¾å¯èƒ½å­˜åœ¨çš„ Audio/Video Section
    if current_audio_section_lines:
        _process_audio_section_languages(current_audio_section_lines,
                                         found_tags)
    if current_video_section_lines:
        _process_video_section_languages(current_video_section_lines,
                                         found_tags)

    # ä¸ºæ‰€æœ‰æ ‡ç­¾æ·»åŠ  tag. å‰ç¼€
    prefixed_tags = set()
    for tag in found_tags:
        if not tag.startswith('tag.'):  # é¿å…é‡å¤æ·»åŠ  tag.
            prefixed_tags.add(f'tag.{tag}')
        else:
            prefixed_tags.add(tag)

    print(f"ä» MediaInfo ä¸­æå–åˆ°çš„æ ‡ç­¾: {list(prefixed_tags)}")
    return list(prefixed_tags)


def _process_audio_section_languages(audio_lines, found_tags):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†éŸ³é¢‘å—ä¸­çš„è¯­è¨€æ£€æµ‹"""
    language = _check_language_in_section(audio_lines)

    if language:
        if language == 'å›½è¯­':
            found_tags.add('å›½è¯­')
        elif language == 'ç²¤è¯­':
            found_tags.add('ç²¤è¯­')
        else:  # å…¶ä»–è¯­è¨€
            found_tags.add(language)
            found_tags.add('å¤–è¯­')
        print(f"   -> ä»éŸ³é¢‘å—ä¸­æå–åˆ°è¯­è¨€: {language}")


def _process_video_section_languages(video_lines, found_tags):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†è§†é¢‘å—ä¸­çš„è¯­è¨€æ£€æµ‹"""
    language = _check_language_in_section(video_lines)
    if language:
        if language == 'å›½è¯­':
            found_tags.add('å›½è¯­')
        elif language == 'ç²¤è¯­':
            found_tags.add('ç²¤è¯­')
        else:  # å…¶ä»–è¯­è¨€
            found_tags.add(language)
            found_tags.add('å¤–è¯­')
        print(f"   -> ä»è§†é¢‘å—ä¸­æå–åˆ°è¯­è¨€: {language}")


def _check_language_in_section(section_lines) -> str | None:
    """
    é€šç”¨å‡½æ•°:æ£€æŸ¥æŒ‡å®š Section å—ä¸­æ˜¯å¦åŒ…å«è¯­è¨€ç›¸å…³æ ‡è¯†ã€‚

    :param section_lines: Section å—çš„æ‰€æœ‰è¡Œ
    :return: å¦‚æœæ£€æµ‹åˆ°è¯­è¨€è¿”å›å…·ä½“è¯­è¨€åç§°,å¦åˆ™è¿”å›None
    """
    language_keywords_map = {
        'å›½è¯­': [
            'ä¸­æ–‡', 'chinese', 'mandarin', 'å›½è¯­', 'æ™®é€šè¯', 'mandrin', 'cmn',
            'mainland'
        ],
        'ç²¤è¯­': ['cantonese', 'ç²¤è¯­', 'å¹¿ä¸œè¯', 'é¦™æ¸¯è¯', 'canton', 'hk', 'hongkong'],
        'å°é…': [
            'å°é…å›½è¯­', 'å°é…', 'tw', 'taiwan', 'twi', 'å°æ¹¾', 'å°è¯­', 'é—½å—è¯­',
            'taiwanese', 'taiwan mandarin'
        ],
        'è‹±è¯­': ['english', 'è‹±è¯­'],
        'æ—¥è¯­': ['japanese', 'æ—¥è¯­'],
        'éŸ©è¯­': ['korean', 'éŸ©è¯­'],
        'æ³•è¯­': ['french', 'æ³•è¯­'],
        'å¾·è¯­': ['german', 'å¾·è¯­'],
        'ä¿„è¯­': ['russian', 'ä¿„è¯­'],
        'å°åœ°è¯­': ['hindi', 'å°åœ°è¯­'],
        'è¥¿ç­ç‰™è¯­': ['spanish', 'è¥¿ç­ç‰™è¯­', 'latin america'],
        'è‘¡è„ç‰™è¯­': ['portuguese', 'è‘¡è„ç‰™è¯­', 'br'],
        'æ„å¤§åˆ©è¯­': ['italian', 'æ„å¤§åˆ©è¯­'],
        'æ³°è¯­': ['thai', 'æ³°è¯­'],
        'é˜¿æ‹‰ä¼¯è¯­': ['arabic', 'é˜¿æ‹‰ä¼¯è¯­', 'sa'],
    }

    for line in section_lines:
        if not line:
            continue
        line_lower = line.lower()

        # ä¼˜å…ˆæ£€æŸ¥ Title: å­—æ®µï¼ˆå› ä¸ºä¸­æ–‡éŸ³è½¨å¸¸åœ¨è¿™é‡Œæ ‡æ³¨ï¼‰
        if 'title' in line_lower and ':' in line_lower:
            # æå– Title å­—æ®µçš„å€¼
            title_match = re.search(r'title\s*:\s*(.+)', line_lower,
                                    re.IGNORECASE)
            if title_match:
                title_value = title_match.group(1).strip()
                # æ£€æŸ¥ Title å€¼ä¸­æ˜¯å¦åŒ…å«è¯­è¨€å…³é”®è¯
                for lang, keywords in language_keywords_map.items():
                    for keyword in keywords:
                        keyword_lower = keyword.lower()
                        if keyword_lower in title_value:
                            return lang

        # å…¶æ¬¡æ£€æŸ¥ Language: å­—æ®µ
        if 'language' in line_lower and ':' in line_lower:
            # æå– Language å­—æ®µçš„å€¼
            lang_match = re.search(r'language\s*:\s*(.+)', line_lower,
                                   re.IGNORECASE)
            if lang_match:
                lang_value = lang_match.group(1).strip()
                # æ£€æŸ¥ Language å€¼ä¸­æ˜¯å¦åŒ…å«è¯­è¨€å…³é”®è¯
                for lang, keywords in language_keywords_map.items():
                    for keyword in keywords:
                        keyword_lower = keyword.lower()
                        if keyword_lower in lang_value:
                            return lang

    return None


def extract_origin_from_description(description_text: str) -> str:
    """
    ä»ç®€ä»‹è¯¦æƒ…ä¸­æå–äº§åœ°ä¿¡æ¯ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦èƒ½åœ¨ global_mappings.yaml çš„ source æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®ã€‚
    å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œåˆ™è®¾ç½®ä¸º'å…¶ä»–'ã€‚

    :param description_text: ç®€ä»‹è¯¦æƒ…æ–‡æœ¬
    :return: äº§åœ°ä¿¡æ¯ï¼Œä¾‹å¦‚ "æ—¥æœ¬"ã€"ä¸­å›½" ç­‰ï¼Œå¦‚æœæ— æ³•æ˜ å°„åˆ™è¿”å› "å…¶ä»–"
    """
    if not description_text:
        return ""

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "â—äº§ã€€ã€€åœ°ã€€æ—¥æœ¬" è¿™ç§æ ¼å¼
    # æ”¯æŒå¤šç§å˜ä½“ï¼šâ—äº§åœ°ã€â—äº§ã€€ã€€åœ°ã€â—å›½ã€€ã€€å®¶ã€â—å›½å®¶åœ°åŒºç­‰
    # ä¿®å¤ï¼šä½¿ç”¨ [^\n\r]+ è€Œä¸æ˜¯ .+? æ¥æ­£ç¡®åŒ¹é…åŒ…å«ç©ºæ ¼çš„äº§åœ°åç§°ï¼ˆå¦‚"ä¸­å›½å¤§é™†"ï¼‰
    patterns = [
        r"[â—â]\s*äº§\s*åœ°\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—äº§åœ° ä¸­å›½å¤§é™†
        r"[â—â]\s*å›½\s*å®¶\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—å›½å®¶ ä¸­å›½å¤§é™†
        r"[â—â]\s*åœ°\s*åŒº\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—åœ°åŒº ä¸­å›½å¤§é™†
        r"[â—â]\s*å›½å®¶åœ°åŒº\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—å›½å®¶åœ°åŒº ä¸­å›½å¤§é™†
        r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶/åœ°åŒº: ä¸­å›½å¤§é™†
        r"åˆ¶ç‰‡å›½å®¶[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶: ä¸­å›½å¤§é™†
        r"å›½å®¶[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… å›½å®¶: ä¸­å›½å¤§é™†
        r"äº§åœ°[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… äº§åœ°: ä¸­å›½å¤§é™†
        r"[äº§]\s*åœ°[:\s]+([^ï¼Œ,\n\r]+)",
        r"[å›½]\s*å®¶[:\s]+([^ï¼Œ,\n\r]+)",
        r"[åœ°]\s*åŒº[:\s]+([^ï¼Œ,\n\r]+)"
    ]

    for pattern in patterns:
        match = re.search(pattern, description_text)
        if match:
            origin = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„å¤šä½™å­—ç¬¦
            origin = re.sub(r'[\[\]ã€ã€‘\(\)]', '', origin).strip()
            # æ·»åŠ é¢å¤–çš„æ¸…ç†æ­¥éª¤ï¼Œå»é™¤å‰ç½®çš„å†’å·ã€ç©ºæ ¼ç­‰å­—ç¬¦
            origin = re.sub(r'^[:\s\u3000]+', '', origin).strip()
            # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦ï¼Œå¦‚" / "ã€","ç­‰
            origin = re.split(r'\s*/\s*|\s*,\s*|\s*;\s*|\s*&\s*',
                              origin)[0].strip()
            print("æå–åˆ°äº§åœ°ä¿¡æ¯:", origin)

            # æ£€æŸ¥äº§åœ°æ˜¯å¦èƒ½åœ¨ global_mappings.yaml çš„ source æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®
            if _check_origin_mapping(origin):
                return origin
            else:
                print(f"äº§åœ° '{origin}' æ— æ³•åœ¨ source æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®ï¼Œè®¾ç½®ä¸º'å…¶ä»–'")
                return "å…¶ä»–"

    return ""


def _check_origin_mapping(origin: str) -> bool:
    """
    æ£€æŸ¥äº§åœ°æ˜¯å¦èƒ½åœ¨ global_mappings.yaml çš„ source æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®ã€‚

    :param origin: äº§åœ°å­—ç¬¦ä¸²
    :return: å¦‚æœèƒ½æ‰¾åˆ°æ˜ å°„è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    try:
        # è¯»å– global_mappings.yaml æ–‡ä»¶
        with open(GLOBAL_MAPPINGS, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # è·å– source æ˜ å°„
        source_mappings = config.get('global_standard_keys',
                                     {}).get('source', {})

        # æ£€æŸ¥äº§åœ°æ˜¯å¦åœ¨æ˜ å°„ä¸­
        if origin in source_mappings:
            print(
                f"äº§åœ° '{origin}' åœ¨ source æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®: {source_mappings[origin]}"
            )
            return True
        else:
            print(f"äº§åœ° '{origin}' åœ¨ source æ˜ å°„ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†é”®")
            return False

    except Exception as e:
        print(f"æ£€æŸ¥äº§åœ°æ˜ å°„æ—¶å‡ºé”™: {e}")
        # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œè¿”å› Trueï¼ˆä¿æŒåŸäº§åœ°ï¼‰
        return True


def extract_resolution_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–åˆ†è¾¨ç‡ä¿¡æ¯ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œä¾‹å¦‚ "720p"ã€"1080p"ã€"2160p" ç­‰
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ Video éƒ¨åˆ†
    video_section_match = re.search(r"Video[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not video_section_match:
        return ""

    video_section = video_section_match.group(0)

    # æŸ¥æ‰¾åˆ†è¾¨ç‡ä¿¡æ¯
    # åŒ¹é…æ ¼å¼å¦‚ï¼šWidth                                 : 1 920 pixels
    #            Height                                : 1 080 pixels
    # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
    width_match = re.search(r"[Ww]idth\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                            video_section)
    height_match = re.search(r"[Hh]eight\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                             video_section)

    width = None
    height = None

    if width_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
        w_groups = width_match.groups()
        if w_groups and len(w_groups) >= 2 and w_groups[1]:
            width = int(f"{w_groups[0]}{w_groups[1]}")
        elif w_groups and len(w_groups) >= 1 and w_groups[0]:
            width = int(w_groups[0])
        else:
            width = None

    if height_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 080" -> "1080"
        h_groups = height_match.groups()
        if h_groups and len(h_groups) >= 2 and h_groups[1]:
            height = int(f"{h_groups[0]}{h_groups[1]}")
        elif h_groups and len(h_groups) >= 1 and h_groups[0]:
            height = int(h_groups[0])
        else:
            height = None

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
    if not width or not height:
        # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾ç±»ä¼¼ "1920 / 1080" çš„æ ¼å¼
        resolution_match = re.search(r"(\d{3,4})\s*/\s*(\d{3,4})",
                                     video_section)
        if resolution_match:
            width = int(resolution_match.group(1))
            height = int(resolution_match.group(2))
        else:
            # æŸ¥æ‰¾å…¶ä»–æ ¼å¼çš„åˆ†è¾¨ç‡ä¿¡æ¯
            other_resolution_match = re.search(r"(\d{3,4})\s*[xX]\s*(\d{3,4})",
                                               mediainfo_text)
            if other_resolution_match:
                width = int(other_resolution_match.group(1))
                height = int(other_resolution_match.group(2))

    # å¦‚æœæ‰¾åˆ°äº†å®½åº¦å’Œé«˜åº¦ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    if width and height:
        # æ ¹æ®é«˜åº¦ç¡®å®šæ ‡å‡†åˆ†è¾¨ç‡
        if height <= 480:
            return "480p"
        elif height <= 576:
            return "576p"
        elif height <= 720:
            return "720p"
        elif height <= 1080:
            return "1080p"
        elif height <= 1440:
            return "1440p"
        elif height <= 2160:
            return "2160p"
        else:
            # å¯¹äºå…¶ä»–éæ ‡å‡†åˆ†è¾¨ç‡ï¼Œè¿”å›åŸå§‹é«˜åº¦åŠ p
            return f"{height}p"

    return ""


def _upload_to_pixhost_direct(image_path: str, api_url: str, params: dict,
                              headers: dict):
    """ç›´æ¥ä¸Šä¼ å›¾ç‰‡åˆ°Pixhost"""
    try:
        with open(image_path, 'rb') as f:
            files = {'img': f}
            print("æ­£åœ¨å‘é€ä¸Šä¼ è¯·æ±‚åˆ° Pixhost...")
            response = requests.post(api_url,
                                     data=params,
                                     files=files,
                                     headers=headers,
                                     timeout=30)

            if response.status_code == 200:
                data = response.json()
                show_url = data.get('show_url')
                print(f"ç›´æ¥ä¸Šä¼ æˆåŠŸï¼å›¾ç‰‡é“¾æ¥: {show_url}")
                return show_url
            else:
                print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                return None
    except FileNotFoundError:
        print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶")
        return None
    except requests.exceptions.SSLError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: SSLè¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: ç½‘ç»œè¿æ¥è¢«é‡ç½®")
        return None
    except requests.exceptions.Timeout:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: è¯·æ±‚è¶…æ—¶")
        return None
    except Exception as e:
        # åªæ‰“å°å¼‚å¸¸ç±»å‹å’Œç®€çŸ­æè¿°ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆ
        error_type = type(e).__name__
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: {error_type}")
        return None


def _get_downloader_proxy_config(downloader_id: str = None):
    """
    æ ¹æ®ä¸‹è½½å™¨IDè·å–ä»£ç†é…ç½®ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :return: ä»£ç†é…ç½®å­—å…¸ï¼Œå¦‚æœä¸éœ€è¦ä»£ç†åˆ™è¿”å›None
    """
    if not downloader_id:
        return None

    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            use_proxy = downloader.get("use_proxy", False)
            if use_proxy:
                host_value = downloader.get('host', '')
                proxy_port = downloader.get('proxy_port', 9090)
                if host_value.startswith(('http://', 'https://')):
                    parsed_url = urlparse(host_value)
                else:
                    parsed_url = urlparse(f"http://{host_value}")
                proxy_ip = parsed_url.hostname
                if not proxy_ip:
                    if '://' in host_value:
                        proxy_ip = host_value.split('://')[1].split(
                            ':')[0].split('/')[0]
                    else:
                        proxy_ip = host_value.split(':')[0]
                proxy_config = {
                    "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                }
                return proxy_config
            break

    return None


def check_intro_completeness(body_text: str) -> dict:
    """
    æ£€æŸ¥ç®€ä»‹æ˜¯å¦å®Œæ•´ï¼ŒåŒ…å«å¿…è¦çš„å½±ç‰‡ä¿¡æ¯å­—æ®µã€‚
    
    :param body_text: ç®€ä»‹æ­£æ–‡å†…å®¹
    :return: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸ {
        "is_complete": bool,      # æ˜¯å¦å®Œæ•´
        "missing_fields": list,   # ç¼ºå¤±çš„å­—æ®µåˆ—è¡¨
        "found_fields": list      # å·²æ‰¾åˆ°çš„å­—æ®µåˆ—è¡¨
    }
    
    ç¤ºä¾‹:
        >>> result = check_intro_completeness(intro_body)
        >>> if not result["is_complete"]:
        >>>     print(f"ç¼ºå°‘å­—æ®µ: {result['missing_fields']}")
    """
    if not body_text:
        return {
            "is_complete": False,
            "missing_fields": ["æ‰€æœ‰å­—æ®µ"],
            "found_fields": []
        }

    # å®šä¹‰å¿…è¦å­—æ®µçš„åŒ¹é…æ¨¡å¼
    # æ¯ä¸ªå­—æ®µå¯ä»¥æœ‰å¤šä¸ªåŒ¹é…æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    required_patterns = {
        "ç‰‡å": [
            r"[â—â]\s*ç‰‡\s*å", r"[â—â]\s*è¯‘\s*å", r"[â—â]\s*æ ‡\s*é¢˜", r"ç‰‡å\s*[:ï¼š]",
            r"è¯‘å\s*[:ï¼š]", r"Title\s*[:ï¼š]"
        ],
        "å¹´ä»£": [
            r"[â—â]\s*å¹´\s*ä»£", r"[â—â]\s*å¹´\s*ä»½", r"å¹´ä»½\s*[:ï¼š]", r"å¹´ä»£\s*[:ï¼š]",
            r"Year\s*[:ï¼š]"
        ],
        "äº§åœ°": [
            r"[â—â]\s*äº§\s*åœ°", r"[â—â]\s*å›½\s*å®¶", r"[â—â]\s*åœ°\s*åŒº",
            r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº\s*[:ï¼š]", r"åˆ¶ç‰‡å›½å®¶\s*[:ï¼š]", r"å›½å®¶\s*[:ï¼š]", r"äº§åœ°\s*[:ï¼š]",
            r"Country\s*[:ï¼š]"
        ],
        "ç±»åˆ«": [
            r"[â—â]\s*ç±»\s*åˆ«", r"[â—â]\s*ç±»\s*å‹", r"ç±»å‹\s*[:ï¼š]", r"ç±»åˆ«\s*[:ï¼š]",
            r"Genre\s*[:ï¼š]"
        ],
        "è¯­è¨€": [r"[â—â]\s*è¯­\s*è¨€", r"è¯­è¨€\s*[:ï¼š]", r"Language\s*[:ï¼š]"],
        "å¯¼æ¼”": [r"[â—â]\s*å¯¼\s*æ¼”", r"å¯¼æ¼”\s*[:ï¼š]", r"Director\s*[:ï¼š]"],
        "ç®€ä»‹": [
            r"[â—â]\s*ç®€\s*ä»‹", r"[â—â]\s*å‰§\s*æƒ…", r"[â—â]\s*å†…\s*å®¹", r"ç®€ä»‹\s*[:ï¼š]",
            r"å‰§æƒ…\s*[:ï¼š]", r"å†…å®¹ç®€ä»‹\s*[:ï¼š]", r"Plot\s*[:ï¼š]", r"Synopsis\s*[:ï¼š]"
        ]
    }

    found_fields = []
    missing_fields = []

    # æ£€æŸ¥æ¯ä¸ªå¿…è¦å­—æ®µ
    for field_name, patterns in required_patterns.items():
        field_found = False
        for pattern in patterns:
            if re.search(pattern, body_text, re.IGNORECASE):
                field_found = True
                break

        if field_found:
            found_fields.append(field_name)
        else:
            missing_fields.append(field_name)

    # åˆ¤æ–­å®Œæ•´æ€§ï¼šå¿…é¡»åŒ…å«ä»¥ä¸‹å…³é”®å­—æ®µ
    # ç‰‡åã€äº§åœ°ã€å¯¼æ¼”ã€ç®€ä»‹ è¿™4ä¸ªå­—æ®µæ˜¯æœ€å…³é”®çš„
    critical_fields = ["ç‰‡å", "äº§åœ°", "å¯¼æ¼”", "ç®€ä»‹"]
    is_complete = all(field in found_fields for field in critical_fields)

    return {
        "is_complete": is_complete,
        "missing_fields": missing_fields,
        "found_fields": found_fields
    }


def is_image_url_valid_robust(url: str) -> bool:
    """
    ä¸€ä¸ªæ›´ç¨³å¥çš„æ–¹æ³•ï¼Œå½“HEADè¯·æ±‚å¤±è´¥æ—¶ï¼Œä¼šå°è¯•ä½¿ç”¨GETè¯·æ±‚ï¼ˆæµå¼ï¼‰è¿›è¡ŒéªŒè¯ã€‚
    å¦‚æœç›´æ¥è¯·æ±‚å¤±è´¥ï¼Œä¼šå°è¯•ä½¿ç”¨å…¨å±€ä»£ç†é‡è¯•ä¸€æ¬¡ã€‚
    """
    if not url:
        return False

    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä¸ä½¿ç”¨ä»£ç†
    try:
        # é¦–å…ˆå°è¯•HEADè¯·æ±‚ï¼Œå…è®¸é‡å®šå‘
        response = requests.head(url, timeout=5, allow_redirects=True)
        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

        # æ£€æŸ¥Content-Type
        content_type = response.headers.get('Content-Type')
        if content_type and content_type.startswith('image/'):
            return True
        else:
            logging.warning(
                f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
            return False

    except requests.exceptions.RequestException:
        # å¦‚æœHEADè¯·æ±‚å¤±è´¥ï¼Œå°è¯•GETè¯·æ±‚
        try:
            response = requests.get(url,
                                    stream=True,
                                    timeout=5,
                                    allow_redirects=True)
            response.raise_for_status()

            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('Content-Type')
            if content_type and content_type.startswith('image/'):
                return True
            else:
                logging.warning(
                    f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
                return False

        except requests.exceptions.RequestException as e:
            logging.warning(f"å›¾ç‰‡é“¾æ¥GETè¯·æ±‚ä¹Ÿå¤±è´¥äº†: {url} - {e}")

            # ä¸ä½¿ç”¨å…¨å±€ä»£ç†é‡è¯•ï¼Œç›´æ¥è¿”å›å¤±è´¥
            return False


def extract_audio_codec_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªéŸ³é¢‘æµçš„æ ¼å¼ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: éŸ³é¢‘æ ¼å¼å­—ç¬¦ä¸² (ä¾‹å¦‚ "DTS", "AC-3", "FLAC")ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª Audio éƒ¨åˆ† (æ”¯æŒ "Audio" å’Œ "Audio #1")
    audio_section_match = re.search(r"Audio(?: #1)?[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not audio_section_match:
        logging.warning("åœ¨MediaInfoä¸­æœªæ‰¾åˆ° 'Audio' éƒ¨åˆ†ã€‚")
        return ""

    audio_section = audio_section_match.group(0)

    # åœ¨ Audio éƒ¨åˆ†æŸ¥æ‰¾ Format
    format_match = re.search(r"Format\s*:\s*(.+)", audio_section)
    if format_match:
        audio_format = format_match.group(1).strip()
        logging.info(f"ä»MediaInfoçš„'Audio'éƒ¨åˆ†æå–åˆ°æ ¼å¼: {audio_format}")
        return audio_format

    logging.warning("åœ¨MediaInfoçš„'Audio'éƒ¨åˆ†æœªæ‰¾åˆ° 'Format' ä¿¡æ¯ã€‚")
    return ""


def _transfer_poster_to_pixhost(poster_url: str) -> str:
    """
    å°†æµ·æŠ¥å›¾ç‰‡è½¬å­˜åˆ°pixhost
    
    :param poster_url: æµ·æŠ¥å›¾ç‰‡URL
    :return: pixhostç›´é“¾URLï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not poster_url:
        return ""

    print(f"å¼€å§‹è½¬å­˜æµ·æŠ¥åˆ°pixhost: {poster_url}")

    try:
        # 1. ä¸‹è½½å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
        headers = {
            'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://movie.douban.com/'
        }

        response = requests.get(poster_url, headers=headers, timeout=30)
        response.raise_for_status()

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if len(response.content) == 0:
            print("   ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶ä¸ºç©º")
            return ""

        if len(response.content) > 10 * 1024 * 1024:
            print("   å›¾ç‰‡æ–‡ä»¶è¿‡å¤§ (>10MB)")
            return ""

        print(f"   å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(response.content)} bytes")

        # 2. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_file = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as f:
                f.write(response.content)
                temp_file = f.name

            print(f"   ä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜: {temp_file}")

            # 3. ä¸Šä¼ åˆ°pixhostï¼Œæ”¯æŒä¸»å¤‡åŸŸååˆ‡æ¢
            api_urls = [
                'http://ptn-proxy.sqing33.dpdns.org/https://api.pixhost.to/images',
                'http://ptn-proxy.1395251710.workers.dev/https://api.pixhost.to/images'
            ]
            params = {'content_type': 0, 'max_th_size': 420}
            upload_headers = {
                'User-Agent':
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
                'Accept': 'application/json'
            }

            upload_response = None
            # å°è¯•ä¸åŒçš„API URL
            for i, api_url in enumerate(api_urls):
                domain_name = "ä¸»åŸŸå" if i == 0 else "å¤‡ç”¨åŸŸå"
                print(f"   å°è¯•ä½¿ç”¨{domain_name}ä¸Šä¼ : {api_url}")

                try:
                    with open(temp_file, 'rb') as f:
                        files = {'img': ('poster.jpg', f, 'image/jpeg')}
                        upload_response = requests.post(api_url,
                                                       data=params,
                                                       files=files,
                                                       headers=upload_headers,
                                                       timeout=30)

                    if upload_response.status_code == 200:
                        print(f"   {domain_name}ä¸Šä¼ æˆåŠŸ")
                        break
                    else:
                        print(f"   {domain_name}ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {upload_response.status_code}")
                        upload_response = None

                except Exception as e:
                    print(f"   {domain_name}ä¸Šä¼ å¼‚å¸¸: {e}")
                    upload_response = None
                    continue

            if not upload_response:
                print("   æ‰€æœ‰APIåŸŸåéƒ½ä¸Šä¼ å¤±è´¥")
                return ""

            if upload_response.status_code == 200:
                data = upload_response.json()
                show_url = data.get('show_url')

                if not show_url:
                    print("   APIæœªè¿”å›æœ‰æ•ˆURL")
                    return ""

                # è½¬æ¢ä¸ºç›´é“¾URL
                direct_url = _convert_pixhost_url_to_direct(show_url)

                if direct_url:
                    print(f"   ä¸Šä¼ æˆåŠŸï¼ç›´é“¾: {direct_url}")
                    return direct_url
                else:
                    print("   URLè½¬æ¢å¤±è´¥")
                    return ""
            else:
                print(f"   ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {upload_response.status_code}")
                return ""

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    print(f"   ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {temp_file}")
                except:
                    pass

    except Exception as e:
        print(f"   è½¬å­˜å¤±è´¥: {type(e).__name__} - {e}")
        return ""


def _convert_pixhost_url_to_direct(show_url: str) -> str:
    """
    å°†pixhostçš„show URLè½¬æ¢ä¸ºç›´é“¾URL
    å‚è€ƒæ²¹çŒ´æ’ä»¶çš„convertToDirectUrlå‡½æ•°
    
    :param show_url: pixhost show URL
    :return: ç›´é“¾URLï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not show_url:
        return ""

    try:
        # æ–¹æ¡ˆ1: ç›´æ¥æ›¿æ¢åŸŸåå’Œè·¯å¾„
        direct_url = show_url.replace(
            'https://pixhost.to/show/',
            'https://img1.pixhost.to/images/').replace(
                'https://pixhost.to/th/', 'https://img1.pixhost.to/images/')

        # ç§»é™¤ç¼©ç•¥å›¾åç¼€ï¼ˆå¦‚ _cover.jpg -> .jpgï¼‰
        direct_url = re.sub(r'_..\.jpg$', '.jpg', direct_url)

        # æ–¹æ¡ˆ2: å¦‚æœæ–¹æ¡ˆ1å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™æå–é‡å»ºURL
        if not direct_url.startswith('https://img1.pixhost.to/images/'):
            match = re.search(r'(\d+)/([^/]+\.(jpg|png|gif))', show_url)
            if match:
                direct_url = f"https://img1.pixhost.to/images/{match.group(1)}/{match.group(2)}"

        # æœ€ç»ˆéªŒè¯
        if re.match(
                r'^https://img1\.pixhost\.to/images/\d+/[^/]+\.(jpg|png|gif)$',
                direct_url):
            return direct_url
        else:
            print(f"   URLæ ¼å¼éªŒè¯å¤±è´¥: {direct_url}")
            return ""

    except Exception as e:
        print(f"   URLè½¬æ¢å¼‚å¸¸: {e}")
        return ""
